#!/usr/bin/env python3
"""
Script to integrate the extended research framework into the Colab notebook.
"""

import json
import sys

def create_markdown_cell(content, cell_id=None):
    """Create a markdown cell."""
    cell = {
        "cell_type": "markdown",
        "metadata": {},
        "source": content.split('\n')
    }
    if cell_id:
        cell["id"] = cell_id
    return cell

def create_code_cell(content, cell_id=None):
    """Create a code cell."""
    cell = {
        "cell_type": "code",
        "execution_count": None,
        "metadata": {},
        "outputs": [],
        "source": content.split('\n')
    }
    if cell_id:
        cell["id"] = cell_id
    return cell

def main():
    # Read the existing notebook
    with open('/home/user/L2-Research-Write-Article/integration_paradox_demo.ipynb', 'r') as f:
        notebook = json.load(f)

    print(f"‚úì Loaded notebook with {len(notebook['cells'])} cells")

    # Define the new cells to add
    new_cells = []

    # Cell 27: Part 2 Header (Markdown)
    new_cells.append(create_markdown_cell(
        "## PART 2: Extended Research Framework\n\n"
        "This section extends the basic Integration Paradox demonstration with:\n"
        "- Failure injection framework\n"
        "- Bottleneck detection system\n"
        "- Comprehensive KPI tracking (fairness, performance, robustness, observability)\n"
        "- Real-time dashboards and visualization\n"
        "- Multi-PoC implementation roadmap",
        "cell-27"
    ))

    # Cell 28: Implementation Roadmap (Markdown)
    new_cells.append(create_markdown_cell(
        "### Section 3: Implementation Roadmap\n\n"
        "This section provides a comprehensive roadmap for implementing multiple PoC pipelines "
        "to demonstrate the Integration Paradox across different AI-enabled SDLC scenarios.\n\n"
        "#### 3.1 PoC Pipeline Variants\n\n"
        "We will implement 4 major pipeline variants:\n\n"
        "1. **PoC 1**: AI-Enabled Automated SE (Current - Extended)\n"
        "2. **PoC 2**: Collaborative AI for SE (Multi-agent collaboration)\n"
        "3. **PoC 3**: Human-Centered AI for SE (Human-in-the-loop)\n"
        "4. **PoC 4**: AI-Assisted MDE (Model-driven engineering)\n\n"
        "#### 3.2 Implementation Phases\n\n"
        "**Phase 1 (Weeks 1-2)**: Failure Injection Framework\n"
        "- Set up failure taxonomy and catalog\n"
        "- Implement failure injection engine\n"
        "- Create cascading simulation capabilities\n\n"
        "**Phase 2 (Weeks 3-4)**: Bottleneck Detection System\n"
        "- Implement detection gap analysis\n"
        "- Build silent propagation detector\n"
        "- Create bottleneck scoring system\n\n"
        "**Phase 3 (Weeks 5-8)**: Instrumentation & Observability\n"
        "- Deploy logging framework (Structured logging)\n"
        "- Set up distributed tracing (OpenTelemetry + Jaeger)\n"
        "- Configure metrics collection (Prometheus)\n\n"
        "**Phase 4 (Weeks 9-12)**: Dashboard & Visualization\n"
        "- Build Grafana dashboards\n"
        "- Create real-time monitoring views\n"
        "- Implement alert systems\n\n"
        "**Phase 5 (Weeks 13-16)**: Multi-PoC Implementation\n"
        "- Implement PoC 2 (Collaborative AI)\n"
        "- Implement PoC 3 (Human-centered)\n"
        "- Implement PoC 4 (MDE)",
        "cell-28"
    ))

    # Cell 29: Failure Injection Framework (Code)
    failure_injection_code = """# ============================================================================
# Failure Injection Framework
# ============================================================================

from enum import Enum
from dataclasses import dataclass
from typing import List, Dict, Any, Optional
import random
import numpy as np
from datetime import datetime

class FailureCategory(Enum):
    DATA_QUALITY = "data_quality"
    MODEL_DRIFT = "model_drift"
    INTEGRATION = "integration"
    INFRASTRUCTURE = "infrastructure"
    HUMAN_ERROR = "human_error"
    SECURITY = "security"

class FailureSeverity(Enum):
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4

@dataclass
class FailureScenario:
    name: str
    category: FailureCategory
    severity: FailureSeverity
    description: str
    affected_agents: List[str]
    propagation_probability: float
    amplification_factor: float
    detection_difficulty: float
    recovery_time_minutes: int
    inject_at_stage: Optional[str] = None

# Create failure catalog
FAILURE_CATALOG = {
    'data_drift': FailureScenario(
        name="Data Distribution Drift",
        category=FailureCategory.DATA_QUALITY,
        severity=FailureSeverity.HIGH,
        description="Input data distribution shifts from training",
        affected_agents=["all"],
        propagation_probability=0.95,
        amplification_factor=1.5,
        detection_difficulty=0.7,
        recovery_time_minutes=60,
        inject_at_stage="requirements"
    ),
    'api_version_mismatch': FailureScenario(
        name="API Version Mismatch",
        category=FailureCategory.INTEGRATION,
        severity=FailureSeverity.CRITICAL,
        description="Upstream service changes API contract",
        affected_agents=["design", "implementation", "testing"],
        propagation_probability=1.0,
        amplification_factor=3.0,
        detection_difficulty=0.4,
        recovery_time_minutes=180,
        inject_at_stage="implementation"
    ),
    'config_error': FailureScenario(
        name="Configuration Error",
        category=FailureCategory.HUMAN_ERROR,
        severity=FailureSeverity.HIGH,
        description="Incorrect configuration parameters",
        affected_agents=["deployment"],
        propagation_probability=0.70,
        amplification_factor=1.6,
        detection_difficulty=0.6,
        recovery_time_minutes=60,
        inject_at_stage="deployment"
    ),
    'security_vulnerability': FailureScenario(
        name="Security Vulnerability",
        category=FailureCategory.SECURITY,
        severity=FailureSeverity.CRITICAL,
        description="Security flaw introduced in design",
        affected_agents=["design", "implementation", "testing"],
        propagation_probability=0.85,
        amplification_factor=2.5,
        detection_difficulty=0.8,
        recovery_time_minutes=240,
        inject_at_stage="design"
    )
}

class FailureInjector:
    def __init__(self, failure_catalog, metrics_collector):
        self.catalog = failure_catalog
        self.metrics = metrics_collector
        self.active_failures = []
        self.injection_history = []

    def inject_failure(self, scenario_name: str, target_agent: str,
                      intensity: float = 1.0) -> Dict[str, Any]:
        scenario = self.catalog[scenario_name]

        injection_event = {
            'timestamp': datetime.now().isoformat(),
            'scenario': scenario.name,
            'target_agent': target_agent,
            'intensity': intensity,
            'category': scenario.category.value,
            'severity': scenario.severity.value
        }

        self.injection_history.append(injection_event)

        effects = self._apply_failure_effects(scenario, target_agent, intensity)
        return effects

    def _apply_failure_effects(self, scenario, target, intensity):
        effects = {
            'performance_degradation': 0.0,
            'error_rate_increase': 0.0,
            'latency_increase': 0.0,
            'output_corruption': 0.0
        }

        if scenario.category == FailureCategory.DATA_QUALITY:
            effects['performance_degradation'] = 0.15 * intensity
            effects['output_corruption'] = 0.25 * intensity
        elif scenario.category == FailureCategory.INTEGRATION:
            effects['error_rate_increase'] = 0.30 * intensity
            effects['latency_increase'] = 0.50 * intensity
        elif scenario.category == FailureCategory.HUMAN_ERROR:
            effects['output_corruption'] = 0.30 * intensity
        elif scenario.category == FailureCategory.SECURITY:
            effects['error_rate_increase'] = 0.20 * intensity
            effects['output_corruption'] = 0.40 * intensity

        for key in effects:
            effects[key] *= scenario.amplification_factor

        return effects

    def simulate_cascade(self, initial_scenario: str, initial_agent: str,
                        pipeline_agents: List[str]) -> List[Dict]:
        scenario = self.catalog[initial_scenario]
        cascade_events = []

        initial_effects = self.inject_failure(initial_scenario, initial_agent, 1.0)
        cascade_events.append({
            'agent': initial_agent,
            'scenario': initial_scenario,
            'effects': initial_effects,
            'propagated': False
        })

        current_intensity = 1.0
        agent_idx = pipeline_agents.index(initial_agent)

        for next_agent in pipeline_agents[agent_idx + 1:]:
            if random.random() < scenario.propagation_probability:
                current_intensity *= scenario.amplification_factor
                propagated_effects = self._apply_failure_effects(
                    scenario, next_agent, current_intensity
                )

                cascade_events.append({
                    'agent': next_agent,
                    'scenario': initial_scenario,
                    'effects': propagated_effects,
                    'propagated': True,
                    'intensity': current_intensity
                })
            else:
                break

        return cascade_events

# Initialize failure injector
failure_injector = FailureInjector(FAILURE_CATALOG, metrics)
print("‚úÖ Failure Injection Framework initialized!")
print(f"üìã {len(FAILURE_CATALOG)} failure scenarios loaded")
for name, scenario in FAILURE_CATALOG.items():
    print(f"   ‚Ä¢ {scenario.name} ({scenario.category.value}, severity: {scenario.severity.value})")"""

    new_cells.append(create_code_cell(failure_injection_code, "cell-29"))

    # Cell 30: Bottleneck Detection (Code)
    bottleneck_code = """# ============================================================================
# Bottleneck Detection System
# ============================================================================

from collections import defaultdict
from typing import List, Dict, Tuple
import numpy as np

class BottleneckDetector:
    def __init__(self, metrics_collector):
        self.metrics = metrics_collector
        self.bottleneck_scores = defaultdict(float)
        self.detection_gaps = []

    def analyze_detection_gaps(self, failure_events: List[Dict],
                              detection_events: List[Dict]) -> List[Dict]:
        \"\"\"Identify failures that slipped through undetected.\"\"\"
        gaps = []
        detected = {d['failure_id']: d for d in detection_events}

        for failure in failure_events:
            if failure['id'] not in detected:
                gap = {
                    'failure_id': failure['id'],
                    'failure_type': failure['scenario'],
                    'agent': failure['agent'],
                    'severity': failure['severity'],
                    'impact_score': self._calculate_impact(failure)
                }
                gaps.append(gap)

        return sorted(gaps, key=lambda x: x['impact_score'], reverse=True)

    def calculate_bottleneck_scores(self, pipeline_stages: List[str],
                                   historical_data: Dict) -> Dict[str, float]:
        \"\"\"Calculate bottleneck risk scores for each pipeline stage.\"\"\"
        scores = {}

        for stage in pipeline_stages:
            score = 0.0

            # Factors weighted by importance
            miss_rate = self._get_detection_miss_rate(stage, historical_data)
            score += miss_rate * 0.30  # 30% weight

            prop_freq = self._get_propagation_frequency(stage, historical_data)
            score += prop_freq * 0.25  # 25% weight

            avg_amplification = self._get_avg_amplification(stage, historical_data)
            score += (avg_amplification - 1.0) * 0.20  # 20% weight

            avg_ttd = self._get_avg_time_to_detection(stage, historical_data)
            score += (avg_ttd / 60.0) * 0.15  # 15% weight

            downstream_impact = self._get_downstream_impact(stage, historical_data)
            score += downstream_impact * 0.10  # 10% weight

            scores[stage] = min(score, 1.0)  # Cap at 1.0

        return scores

    def identify_integration_boundaries_at_risk(self, pipeline_agents: List[str],
                                               failure_data: Dict) -> List[Tuple]:
        \"\"\"Identify agent boundaries with highest failure propagation risk.\"\"\"
        boundaries = []

        for i in range(len(pipeline_agents) - 1):
            source = pipeline_agents[i]
            target = pipeline_agents[i + 1]
            risk_score = self._calculate_boundary_risk(source, target, failure_data)
            boundaries.append((source, target, risk_score))

        return sorted(boundaries, key=lambda x: x[2], reverse=True)

    def recommend_monitoring_improvements(self, bottlenecks: Dict,
                                         gaps: List[Dict]) -> List[Dict]:
        \"\"\"Generate monitoring improvement recommendations.\"\"\"
        recommendations = []

        for stage, score in sorted(bottlenecks.items(), key=lambda x: x[1], reverse=True):
            if score > 0.5:  # Only high-risk stages
                rec = {
                    'stage': stage,
                    'risk_score': score,
                    'recommendations': []
                }

                stage_gaps = [g for g in gaps if g['agent'] == stage]
                if stage_gaps:
                    failure_types = set(g['failure_type'] for g in stage_gaps)
                    for ft in failure_types:
                        rec['recommendations'].append({
                            'type': 'add_detector',
                            'failure_type': ft,
                            'priority': 'high' if score > 0.7 else 'medium'
                        })

                # Add tracing recommendation for high propagation
                if score > 0.7:
                    rec['recommendations'].append({
                        'type': 'add_distributed_tracing',
                        'failure_type': 'all',
                        'priority': 'high'
                    })

                recommendations.append(rec)

        return recommendations

    def _get_detection_miss_rate(self, stage, data):
        \"\"\"Simulated detection miss rate (would use historical data).\"\"\"
        return 0.15

    def _get_propagation_frequency(self, stage, data):
        \"\"\"Simulated propagation frequency.\"\"\"
        return 0.75

    def _get_avg_amplification(self, stage, data):
        \"\"\"Simulated average amplification factor.\"\"\"
        return 1.5

    def _get_avg_time_to_detection(self, stage, data):
        \"\"\"Simulated average time to detection (seconds).\"\"\"
        return 180.0

    def _get_downstream_impact(self, stage, data):
        \"\"\"Simulated downstream impact score.\"\"\"
        return 0.6

    def _calculate_boundary_risk(self, source, target, data):
        \"\"\"Calculate risk at boundary between two agents.\"\"\"
        return 0.7

    def _calculate_impact(self, failure):
        \"\"\"Calculate impact score for a failure.\"\"\"
        severity_weights = {1: 0.25, 2: 0.5, 3: 0.75, 4: 1.0}
        return severity_weights.get(failure['severity'], 0.5)

# Initialize bottleneck detector
bottleneck_detector = BottleneckDetector(metrics)
print("‚úÖ Bottleneck Detection System initialized!")"""

    new_cells.append(create_code_cell(bottleneck_code, "cell-30"))

    # Cell 31: KPI Tracking (Code)
    kpi_code = """# ============================================================================
# Comprehensive KPI Tracking Framework
# ============================================================================

class KPITracker:
    \"\"\"Track comprehensive KPIs across 4 categories: Fairness, Performance, Robustness, Observability.\"\"\"

    def __init__(self):
        self.fairness_metrics = {}
        self.performance_metrics = {}
        self.robustness_metrics = {}
        self.observability_metrics = {}

    def track_fairness(self, agent_name: str, predictions,
                      protected_attributes, labels):
        \"\"\"Track fairness metrics: demographic parity, equalized odds, disparate impact.\"\"\"
        metrics = {}

        # Demographic Parity: P(Y=1|A=a) should be equal across groups
        for attr in set(protected_attributes):
            mask = [p == attr for p in protected_attributes]
            if sum(mask) > 0:
                pos_rate = sum([1 for i, m in enumerate(mask) if m and predictions[i] == 1]) / sum(mask)
                metrics[f'demographic_parity_{attr}'] = pos_rate

        # Disparate Impact: ratio of positive rates
        groups = list(set(protected_attributes))
        if len(groups) >= 2:
            rates = [metrics.get(f'demographic_parity_{g}', 0) for g in groups]
            if max(rates) > 0:
                metrics['disparate_impact'] = min(rates) / max(rates)

        self.fairness_metrics[agent_name] = metrics
        return metrics

    def track_performance(self, agent_name: str, predictions, ground_truth):
        \"\"\"Track performance metrics: accuracy, precision, recall, F1, AUC-ROC.\"\"\"
        try:
            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

            metrics = {
                'accuracy': accuracy_score(ground_truth, predictions),
                'precision': precision_score(ground_truth, predictions, average='weighted', zero_division=0),
                'recall': recall_score(ground_truth, predictions, average='weighted', zero_division=0),
                'f1_score': f1_score(ground_truth, predictions, average='weighted', zero_division=0)
            }
        except ImportError:
            # Fallback if sklearn not available
            metrics = {
                'accuracy': sum([1 for p, g in zip(predictions, ground_truth) if p == g]) / len(predictions),
                'note': 'sklearn unavailable - limited metrics'
            }

        self.performance_metrics[agent_name] = metrics
        return metrics

    def track_robustness(self, agent_name: str, predictions_baseline,
                        predictions_perturbed):
        \"\"\"Track robustness metrics: sensitivity to perturbations, calibration, OOD detection.\"\"\"
        import numpy as np

        # Sensitivity to perturbations
        diff = np.abs(np.array(predictions_baseline) - np.array(predictions_perturbed))

        metrics = {
            'mean_sensitivity': float(np.mean(diff)),
            'max_sensitivity': float(np.max(diff)),
            'std_sensitivity': float(np.std(diff)),
            'robust_prediction_rate': float(np.mean(diff < 0.1))  # % predictions that changed <10%
        }

        self.robustness_metrics[agent_name] = metrics
        return metrics

    def track_observability(self, agent_name: str, latency_ms: float,
                          error_count: int, total_requests: int):
        \"\"\"Track observability metrics: latency (p50, p95, p99), error rates, MTBF, MTTR.\"\"\"
        metrics = {
            'avg_latency_ms': latency_ms,
            'error_rate': error_count / total_requests if total_requests > 0 else 0,
            'availability': 1.0 - (error_count / total_requests) if total_requests > 0 else 1.0,
            'throughput_rps': total_requests / 60.0  # Assuming 1-minute window
        }

        self.observability_metrics[agent_name] = metrics
        return metrics

    def generate_kpi_report(self) -> str:
        \"\"\"Generate comprehensive KPI report across all categories.\"\"\"
        report = "\\n" + "="*70 + "\\n"
        report += "                 COMPREHENSIVE KPI REPORT\\n"
        report += "="*70 + "\\n\\n"

        report += "üìä FAIRNESS METRICS\\n"
        report += "-" * 70 + "\\n"
        if self.fairness_metrics:
            for agent, metrics in self.fairness_metrics.items():
                report += f"  {agent}:\\n"
                for metric, value in metrics.items():
                    report += f"    {metric}: {value:.4f}\\n"
        else:
            report += "  No fairness metrics tracked yet\\n"

        report += "\\nüìà PERFORMANCE METRICS\\n"
        report += "-" * 70 + "\\n"
        if self.performance_metrics:
            for agent, metrics in self.performance_metrics.items():
                report += f"  {agent}:\\n"
                for metric, value in metrics.items():
                    if isinstance(value, (int, float)):
                        report += f"    {metric}: {value:.4f}\\n"
                    else:
                        report += f"    {metric}: {value}\\n"
        else:
            report += "  No performance metrics tracked yet\\n"

        report += "\\nüõ°Ô∏è  ROBUSTNESS METRICS\\n"
        report += "-" * 70 + "\\n"
        if self.robustness_metrics:
            for agent, metrics in self.robustness_metrics.items():
                report += f"  {agent}:\\n"
                for metric, value in metrics.items():
                    report += f"    {metric}: {value:.4f}\\n"
        else:
            report += "  No robustness metrics tracked yet\\n"

        report += "\\nüëÅÔ∏è  OBSERVABILITY METRICS\\n"
        report += "-" * 70 + "\\n"
        if self.observability_metrics:
            for agent, metrics in self.observability_metrics.items():
                report += f"  {agent}:\\n"
                for metric, value in metrics.items():
                    report += f"    {metric}: {value:.4f}\\n"
        else:
            report += "  No observability metrics tracked yet\\n"

        return report

# Initialize KPI tracker
kpi_tracker = KPITracker()
print("‚úÖ Comprehensive KPI Tracking initialized!")
print("üìä Tracking 4 KPI categories: Fairness, Performance, Robustness, Observability")"""

    new_cells.append(create_code_cell(kpi_code, "cell-31"))

    # Cell 32: Dashboard (Code)
    dashboard_code = """# ============================================================================
# Real-Time Dashboard & Visualization
# ============================================================================

import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.express as px

class IntegrationParadoxDashboard:
    \"\"\"Create interactive dashboards for Integration Paradox analysis.\"\"\"

    def __init__(self, metrics_collector, kpi_tracker, failure_injector):
        self.metrics = metrics_collector
        self.kpis = kpi_tracker
        self.failures = failure_injector

    def create_main_dashboard(self):
        \"\"\"Create comprehensive 2x2 dashboard with key metrics.\"\"\"
        # Create 2x2 subplot dashboard
        fig = make_subplots(
            rows=2, cols=2,
            subplot_titles=(
                'Integration Gap Over Time',
                'Error Propagation Network',
                'Failure Injection Timeline',
                'Agent Performance Comparison'
            ),
            specs=[
                [{'type': 'scatter'}, {'type': 'scatter'}],
                [{'type': 'bar'}, {'type': 'bar'}]
            ]
        )

        # Plot 1: Integration Gap Trend
        isolated = list(self.metrics.calculate_isolated_accuracy().values())
        system = self.metrics.calculate_system_accuracy()

        if isolated:
            fig.add_trace(
                go.Scatter(
                    x=list(range(len(isolated))),
                    y=[i*100 for i in isolated],
                    name='Isolated Accuracy',
                    mode='lines+markers',
                    line=dict(color='green', width=2)
                ),
                row=1, col=1
            )

            fig.add_trace(
                go.Scatter(
                    x=list(range(len(isolated))),
                    y=[system*100] * len(isolated),
                    name='System Accuracy',
                    mode='lines',
                    line=dict(color='red', width=2, dash='dash')
                ),
                row=1, col=1
            )

        # Plot 2: Error Propagation Network
        if self.metrics.error_propagation:
            sources = [e['source'] for e in self.metrics.error_propagation]
            targets = [e['target'] for e in self.metrics.error_propagation]

            # Create unique positions for agents
            unique_agents = list(set(sources + targets))
            agent_positions = {agent: i for i, agent in enumerate(unique_agents)}

            fig.add_trace(
                go.Scatter(
                    x=[agent_positions[s] for s in sources],
                    y=[agent_positions[t] for t in targets],
                    mode='markers',
                    marker=dict(size=10, color='red'),
                    name='Error Propagations'
                ),
                row=1, col=2
            )

        # Plot 3: Failure Injection Timeline
        if self.failures.injection_history:
            times = list(range(len(self.failures.injection_history)))
            severities = [e['severity'] for e in self.failures.injection_history]
            scenarios = [e['scenario'] for e in self.failures.injection_history]

            fig.add_trace(
                go.Bar(
                    x=times,
                    y=severities,
                    name='Failure Severity',
                    text=scenarios,
                    hovertemplate='%{text}<br>Severity: %{y}<extra></extra>'
                ),
                row=2, col=1
            )

        # Plot 4: Agent Performance Comparison
        if self.metrics.agent_results:
            agent_names = list(set([r['agent'] for r in self.metrics.agent_results]))
            success_rates = []

            for agent in agent_names:
                agent_results = [r for r in self.metrics.agent_results if r['agent'] == agent]
                success_rate = sum(1 for r in agent_results if r['success']) / len(agent_results) if agent_results else 0
                success_rates.append(success_rate * 100)

            fig.add_trace(
                go.Bar(
                    x=agent_names,
                    y=success_rates,
                    name='Success Rate',
                    marker=dict(color=success_rates, colorscale='RdYlGn', cmin=0, cmax=100)
                ),
                row=2, col=2
            )

        # Update layout
        fig.update_layout(
            height=800,
            title_text="Integration Paradox Real-Time Dashboard",
            showlegend=True
        )

        fig.update_xaxes(title_text="Agent Index", row=1, col=1)
        fig.update_yaxes(title_text="Accuracy (%)", row=1, col=1)

        fig.update_xaxes(title_text="Source Agent", row=1, col=2)
        fig.update_yaxes(title_text="Target Agent", row=1, col=2)

        fig.update_xaxes(title_text="Injection Event", row=2, col=1)
        fig.update_yaxes(title_text="Severity (1-4)", row=2, col=1)

        fig.update_xaxes(title_text="Agent", row=2, col=2)
        fig.update_yaxes(title_text="Success Rate (%)", row=2, col=2)

        return fig

    def create_bottleneck_heatmap(self, pipeline_stages: List[str]):
        \"\"\"Create bottleneck analysis heatmap.\"\"\"
        import numpy as np

        # Mock data for demonstration (would use real historical data)
        metrics_grid = np.random.rand(len(pipeline_stages), 5)

        fig = px.imshow(
            metrics_grid,
            x=['Detection Miss', 'Propagation Freq', 'Amplification',
               'Time to Detect', 'Downstream Impact'],
            y=pipeline_stages,
            color_continuous_scale='RdYlGn_r',
            title='Pipeline Bottleneck Analysis Heatmap',
            labels=dict(x="Risk Factor", y="Pipeline Stage", color="Risk Score")
        )

        fig.update_layout(height=600)
        return fig

    def create_cascade_visualization(self, cascade_events: List[Dict]):
        \"\"\"Visualize error cascade through pipeline.\"\"\"
        fig = go.Figure()

        agents = [e['agent'] for e in cascade_events]
        intensities = [e.get('intensity', 1.0) for e in cascade_events]

        fig.add_trace(go.Scatter(
            x=list(range(len(agents))),
            y=intensities,
            mode='lines+markers',
            name='Error Intensity',
            line=dict(color='red', width=3),
            marker=dict(size=12),
            text=agents,
            hovertemplate='%{text}<br>Intensity: %{y:.2f}x<extra></extra>'
        ))

        fig.update_layout(
            title='Error Cascade Amplification Through Pipeline',
            xaxis_title='Pipeline Stage',
            yaxis_title='Error Intensity (Amplification Factor)',
            xaxis=dict(ticktext=agents, tickvals=list(range(len(agents)))),
            height=500
        )

        return fig

# Initialize dashboard
dashboard = IntegrationParadoxDashboard(metrics, kpi_tracker, failure_injector)
print("‚úÖ Interactive Dashboard initialized!")
print("üìä Use dashboard.create_main_dashboard() to visualize results")"""

    new_cells.append(create_code_cell(dashboard_code, "cell-32"))

    # Cell 33: Demonstration (Code)
    demo_code = """# ============================================================================
# DEMONSTRATION: Simulating Cascading Failures
# ============================================================================

print("\\n" + "="*70)
print("         CASCADING FAILURE SIMULATION DEMONSTRATION")
print("="*70 + "\\n")

# Define pipeline agents
pipeline_agents = [
    "Requirements Agent",
    "Design Agent",
    "Implementation Agent",
    "Testing Agent",
    "Deployment Agent"
]

# Simulate data drift failure starting at requirements
print("üî¥ Injecting 'data_drift' failure at Requirements Agent...")
cascade = failure_injector.simulate_cascade(
    initial_scenario='data_drift',
    initial_agent='Requirements Agent',
    pipeline_agents=pipeline_agents
)

print(f"\\nüìä Cascade Results: {len(cascade)} stages affected")
print("-" * 70)

for i, event in enumerate(cascade):
    propagated_marker = "üî¥ PROPAGATED" if event.get('propagated') else "üü¢ INITIAL"
    intensity = event.get('intensity', 1.0)

    print(f"\\nStage {i+1}: {event['agent']}")
    print(f"  Status: {propagated_marker}")
    print(f"  Intensity: {intensity:.2f}x")
    print(f"  Effects:")

    for effect_type, value in event.get('effects', {}).items():
        if value > 0:
            print(f"    - {effect_type}: {value:.2%}")

# Analyze bottlenecks
print("\\n" + "="*70)
print("         BOTTLENECK ANALYSIS")
print("="*70 + "\\n")

bottleneck_scores = bottleneck_detector.calculate_bottleneck_scores(
    pipeline_stages=pipeline_agents,
    historical_data={}
)

print("üéØ Bottleneck Risk Scores (0.0 = low, 1.0 = critical):\\n")
for stage, score in sorted(bottleneck_scores.items(), key=lambda x: x[1], reverse=True):
    risk_level = "üî¥ CRITICAL" if score > 0.7 else "üü° HIGH" if score > 0.5 else "üü¢ MEDIUM"
    print(f"  {stage:25s}: {score:.2f} {risk_level}")

# Identify high-risk boundaries
print("\\nüîç High-Risk Integration Boundaries:\\n")
boundaries = bottleneck_detector.identify_integration_boundaries_at_risk(
    pipeline_agents=pipeline_agents,
    failure_data={}
)

for source, target, risk in boundaries[:3]:  # Top 3
    print(f"  {source} ‚Üí {target}: Risk = {risk:.2f}")

# Generate recommendations
print("\\n" + "="*70)
print("         MONITORING RECOMMENDATIONS")
print("="*70 + "\\n")

recommendations = bottleneck_detector.recommend_monitoring_improvements(
    bottlenecks=bottleneck_scores,
    gaps=[]
)

for rec in recommendations:
    print(f"üìç {rec['stage']} (Risk: {rec['risk_score']:.2f})")
    for r in rec['recommendations']:
        print(f"   ‚Üí {r['type']}: {r['priority']} priority")

# Visualize cascade
print("\\nüìä Generating cascade visualization...")
cascade_fig = dashboard.create_cascade_visualization(cascade)
cascade_fig.show()

# Generate main dashboard
print("\\nüìä Generating comprehensive dashboard...")
main_dashboard = dashboard.create_main_dashboard()
main_dashboard.show()

print("\\n‚úÖ Demonstration complete!")"""

    new_cells.append(create_code_cell(demo_code, "cell-33"))

    # Cell 34: Export Framework (Code)
    export_code = """# ============================================================================
# Export Complete Research Framework
# ============================================================================

def export_research_framework():
    \"\"\"Export all framework data for analysis and reporting.\"\"\"

    framework_data = {
        'metadata': {
            'framework_version': '2.0',
            'export_timestamp': datetime.now().isoformat(),
            'poc_variants': 4,
            'failure_scenarios': len(FAILURE_CATALOG)
        },
        'metrics': {
            'integration_paradox': {
                'isolated_accuracy': metrics.calculate_isolated_accuracy(),
                'system_accuracy': metrics.calculate_system_accuracy(),
                'integration_gap_percent': metrics.calculate_integration_gap()
            },
            'kpis': {
                'fairness': kpi_tracker.fairness_metrics,
                'performance': kpi_tracker.performance_metrics,
                'robustness': kpi_tracker.robustness_metrics,
                'observability': kpi_tracker.observability_metrics
            },
            'bottlenecks': bottleneck_scores
        },
        'failures': {
            'catalog': {k: {
                'name': v.name,
                'category': v.category.value,
                'severity': v.severity.value,
                'propagation_probability': v.propagation_probability,
                'amplification_factor': v.amplification_factor
            } for k, v in FAILURE_CATALOG.items()},
            'injection_history': failure_injector.injection_history
        },
        'cascade_simulation': cascade,
        'recommendations': recommendations
    }

    # Save to JSON
    with open('complete_research_framework.json', 'w') as f:
        json.dump(framework_data, f, indent=2)

    print("‚úÖ Complete research framework exported!")
    print("üìÅ Files created:")
    print("   - complete_research_framework.json")

    return framework_data

# Execute export
framework_data = export_research_framework()

# Display summary
print("\\n" + "="*70)
print("         COMPLETE FRAMEWORK SUMMARY")
print("="*70)
print(f"\\nüì¶ Framework Version: {framework_data['metadata']['framework_version']}")
print(f"üéØ PoC Variants: {framework_data['metadata']['poc_variants']}")
print(f"‚ö†Ô∏è  Failure Scenarios: {framework_data['metadata']['failure_scenarios']}")
print(f"üìä Cascade Events: {len(framework_data['cascade_simulation'])}")
print(f"üîç Bottlenecks Identified: {len(framework_data['metrics']['bottlenecks'])}")
print(f"üí° Recommendations Generated: {len(framework_data['recommendations'])}")

# Generate comprehensive reports
print("\\n" + "="*70)
print(kpi_tracker.generate_kpi_report())

# Create bottleneck heatmap
print("\\nüìä Generating bottleneck heatmap...")
heatmap_fig = dashboard.create_bottleneck_heatmap(pipeline_agents)
heatmap_fig.show()

print("\\n" + "="*70)
print("‚úÖ EXTENDED RESEARCH FRAMEWORK COMPLETE!")
print("="*70)
print("\\nNext steps:")
print("1. Implement additional PoC variants (Collaborative AI, Human-centered, MDE)")
print("2. Deploy real instrumentation (OpenTelemetry, Prometheus, Grafana)")
print("3. Run experiments with real failure injection")
print("4. Collect production metrics and refine KPIs")"""

    new_cells.append(create_code_cell(export_code, "cell-34"))

    # Add all new cells to the notebook
    notebook['cells'].extend(new_cells)

    # Save the updated notebook
    with open('/home/user/L2-Research-Write-Article/integration_paradox_demo.ipynb', 'w') as f:
        json.dump(notebook, f, indent=2)

    print(f"\n‚úÖ Successfully added {len(new_cells)} cells to the notebook!")
    print(f"üìä Total cells: {len(notebook['cells'])}")
    print("\nNew cells added:")
    for i, cell in enumerate(new_cells, start=27):
        cell_type = cell['cell_type']
        preview = str(cell['source'][0])[:60] if cell['source'] else ""
        print(f"  Cell {i}: [{cell_type}] {preview}...")

if __name__ == "__main__":
    main()
