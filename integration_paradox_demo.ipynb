{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# The Integration Paradox: CrewAI Multi-Agent SDLC Demonstration\n",
        "\n",
        "This notebook demonstrates the Integration Paradox through a multi-agent AI system implementing a complete SDLC pipeline.\n",
        "\n",
        "## Architecture\n",
        "```\n",
        "Requirements Agent (Claude) -> Design Agent (GPT-4) -> Implementation Agent (Codex) \n",
        "  -> Testing Agent (StarCoder) -> Deployment Agent (GPT-3.5-Turbo)\n",
        "```\n",
        "\n",
        "## Hypothesis\n",
        "- **Isolated Success Rate**: Each agent achieves >90% on individual tasks\n",
        "- **Composed Success Rate**: System achieves <35% due to cascading errors\n",
        "- **Error Amplification**: Quadratic error compounding across agent boundaries"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup & Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q crewai==0.28.8 crewai_tools==0.1.6 langchain_community==0.0.29\n",
        "!pip install -q anthropic openai huggingface_hub langchain-anthropic langchain-openai\n",
        "!pip install -q matplotlib pandas numpy seaborn plotly\n",
        "\n",
        "print(\"\u2705 All dependencies installed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. API Configuration\n",
        "\n",
        "### Required API Keys (store in Colab Secrets):\n",
        "- `OPENAI_API_KEY`: For GPT-4, Codex, and GPT-3.5-Turbo\n",
        "- `ANTHROPIC_API_KEY`: For Claude (Requirements Agent)\n",
        "- `HUGGINGFACE_API_KEY`: For StarCoder (Testing Agent)\n",
        "\n",
        "### How to add secrets:\n",
        "1. Click the \ud83d\udd11 key icon on the left sidebar\n",
        "2. Click \"+ New secret\"\n",
        "3. Add each key with exact names above\n",
        "4. Toggle \"Notebook access\" ON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "from typing import Dict, List, Tuple\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Configure API keys from Colab Secrets\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get('ANTHROPIC_API_KEY')\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = userdata.get('HUGGINGFACE_API_KEY')\n",
        "\n",
        "print(\"\u2705 API keys configured successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Import CrewAI and Configure LLM Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crewai import Agent, Task, Crew, Process\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "from langchain_community.llms import HuggingFaceHub\n",
        "\n",
        "# Initialize different LLM models for each agent\n",
        "# Requirements Agent: Claude 3.5 Sonnet (best for analysis and requirements)\n",
        "claude_llm = ChatAnthropic(\n",
        "    model=\"claude-3-5-sonnet-20241022\",\n",
        "    temperature=0.3,\n",
        "    anthropic_api_key=os.environ[\"ANTHROPIC_API_KEY\"]\n",
        ")\n",
        "\n",
        "# Design Agent: GPT-4 (best for architecture and design)\n",
        "gpt4_llm = ChatOpenAI(\n",
        "    model=\"gpt-4-turbo-preview\",\n",
        "    temperature=0.4,\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "\n",
        "# Implementation Agent: GPT-4 (Codex deprecated, using GPT-4 for code generation)\n",
        "codex_llm = ChatOpenAI(\n",
        "    model=\"gpt-4\",\n",
        "    temperature=0.2,\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "\n",
        "# Testing Agent: StarCoder via HuggingFace\n",
        "starcoder_llm = HuggingFaceHub(\n",
        "    repo_id=\"bigcode/starcoder\",\n",
        "    model_kwargs={\"temperature\": 0.3, \"max_length\": 2000},\n",
        "    huggingfacehub_api_token=os.environ[\"HUGGINGFACE_API_KEY\"]\n",
        ")\n",
        "\n",
        "# Deployment Agent: GPT-3.5-Turbo (cost-effective for deployment tasks)\n",
        "deployment_llm = ChatOpenAI(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    temperature=0.3,\n",
        "    openai_api_key=os.environ[\"OPENAI_API_KEY\"]\n",
        ")\n",
        "\n",
        "print(\"\u2705 All LLM models initialized successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Metrics Tracking Framework\n",
        "\n",
        "This class tracks metrics to demonstrate the Integration Paradox."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class IntegrationMetrics:\n",
        "    \"\"\"Track metrics to demonstrate the Integration Paradox.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.agent_results = []\n",
        "        self.error_propagation = []\n",
        "        self.timestamps = []\n",
        "        \n",
        "    def record_agent_output(self, agent_name: str, task_name: str, \n",
        "                           output: str, success: bool, errors: List[str]):\n",
        "        \"\"\"Record individual agent performance.\"\"\"\n",
        "        self.agent_results.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'agent': agent_name,\n",
        "            'task': task_name,\n",
        "            'output_length': len(output),\n",
        "            'success': success,\n",
        "            'errors': errors,\n",
        "            'error_count': len(errors)\n",
        "        })\n",
        "        \n",
        "    def record_error_propagation(self, source_agent: str, target_agent: str, \n",
        "                                error_type: str, amplified: bool):\n",
        "        \"\"\"Track how errors propagate between agents.\"\"\"\n",
        "        self.error_propagation.append({\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'source': source_agent,\n",
        "            'target': target_agent,\n",
        "            'error_type': error_type,\n",
        "            'amplified': amplified\n",
        "        })\n",
        "    \n",
        "    def calculate_isolated_accuracy(self) -> Dict[str, float]:\n",
        "        \"\"\"Calculate individual agent success rates.\"\"\"\n",
        "        df = pd.DataFrame(self.agent_results)\n",
        "        if df.empty:\n",
        "            return {}\n",
        "        return df.groupby('agent')['success'].mean().to_dict()\n",
        "    \n",
        "    def calculate_system_accuracy(self) -> float:\n",
        "        \"\"\"Calculate end-to-end system success rate.\"\"\"\n",
        "        if not self.agent_results:\n",
        "            return 0.0\n",
        "        # System succeeds only if ALL agents succeed\n",
        "        all_success = all(r['success'] for r in self.agent_results)\n",
        "        return 1.0 if all_success else 0.0\n",
        "    \n",
        "    def calculate_integration_gap(self) -> float:\n",
        "        \"\"\"Calculate the Integration Paradox gap (92% in the paper).\"\"\"\n",
        "        isolated = self.calculate_isolated_accuracy()\n",
        "        if not isolated:\n",
        "            return 0.0\n",
        "        avg_isolated = sum(isolated.values()) / len(isolated)\n",
        "        system_accuracy = self.calculate_system_accuracy()\n",
        "        return (avg_isolated - system_accuracy) * 100  # Return as percentage\n",
        "    \n",
        "    def generate_report(self) -> str:\n",
        "        \"\"\"Generate comprehensive metrics report.\"\"\"\n",
        "        isolated = self.calculate_isolated_accuracy()\n",
        "        system = self.calculate_system_accuracy()\n",
        "        gap = self.calculate_integration_gap()\n",
        "        \n",
        "        report = f\"\"\"\n",
        "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
        "\u2551     INTEGRATION PARADOX DEMONSTRATION RESULTS             \u2551\n",
        "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
        "\n",
        "\ud83d\udcca ISOLATED AGENT ACCURACY (Component-Level):\n",
        "\"\"\"\n",
        "        for agent, accuracy in isolated.items():\n",
        "            report += f\"   \u2022 {agent:25s}: {accuracy*100:5.1f}%\\n\"\n",
        "        \n",
        "        avg_isolated = sum(isolated.values()) / len(isolated) if isolated else 0\n",
        "        report += f\"\\n   Average Isolated Accuracy: {avg_isolated*100:.1f}%\\n\"\n",
        "        \n",
        "        report += f\"\"\"\n",
        "\ud83d\udd17 COMPOSED SYSTEM ACCURACY (Integration-Level):\n",
        "   End-to-End Success Rate: {system*100:.1f}%\n",
        "\n",
        "\u26a0\ufe0f  INTEGRATION PARADOX GAP:\n",
        "   Performance Degradation: {gap:.1f}%\n",
        "   \n",
        "\ud83d\udcc8 ERROR PROPAGATION:\n",
        "   Total Cascading Errors: {len(self.error_propagation)}\n",
        "   Amplified Errors: {sum(1 for e in self.error_propagation if e['amplified'])}\n",
        "\n",
        "\ud83d\udca1 INTERPRETATION:\n",
        "\"\"\"\n",
        "        if gap > 50:\n",
        "            report += \"   \u2713 PARADOX CONFIRMED: {:.0f}% gap demonstrates that reliable\\n\".format(gap)\n",
        "            report += \"     components compose into unreliable systems.\\n\"\n",
        "        else:\n",
        "            report += \"   \u2139 Integration gap: {:.0f}% (further testing needed)\\n\".format(gap)\n",
        "        \n",
        "        return report\n",
        "    \n",
        "    def visualize_results(self):\n",
        "        \"\"\"Create visualizations of the Integration Paradox.\"\"\"\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "        fig.suptitle('Integration Paradox: Visualization', fontsize=16, fontweight='bold')\n",
        "        \n",
        "        # 1. Isolated vs System Accuracy\n",
        "        isolated = self.calculate_isolated_accuracy()\n",
        "        system = self.calculate_system_accuracy()\n",
        "        \n",
        "        agents = list(isolated.keys()) + ['System\\n(Composed)']\n",
        "        accuracies = list(isolated.values()) + [system]\n",
        "        colors = ['green'] * len(isolated) + ['red']\n",
        "        \n",
        "        axes[0, 0].bar(range(len(agents)), [a*100 for a in accuracies], color=colors, alpha=0.7)\n",
        "        axes[0, 0].set_xticks(range(len(agents)))\n",
        "        axes[0, 0].set_xticklabels(agents, rotation=45, ha='right')\n",
        "        axes[0, 0].set_ylabel('Accuracy (%)')\n",
        "        axes[0, 0].set_title('Component vs System Accuracy')\n",
        "        axes[0, 0].axhline(y=90, color='blue', linestyle='--', label='90% Target')\n",
        "        axes[0, 0].legend()\n",
        "        axes[0, 0].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # 2. Error Propagation Flow\n",
        "        if self.error_propagation:\n",
        "            df_errors = pd.DataFrame(self.error_propagation)\n",
        "            error_counts = df_errors.groupby('source').size()\n",
        "            axes[0, 1].bar(error_counts.index, error_counts.values, color='orange', alpha=0.7)\n",
        "            axes[0, 1].set_xlabel('Source Agent')\n",
        "            axes[0, 1].set_ylabel('Errors Generated')\n",
        "            axes[0, 1].set_title('Error Generation by Agent')\n",
        "            axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "            axes[0, 1].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        # 3. Error Types Distribution\n",
        "        if self.agent_results:\n",
        "            df_results = pd.DataFrame(self.agent_results)\n",
        "            error_counts_by_agent = df_results.groupby('agent')['error_count'].sum()\n",
        "            axes[1, 0].barh(error_counts_by_agent.index, error_counts_by_agent.values, \n",
        "                           color='crimson', alpha=0.7)\n",
        "            axes[1, 0].set_xlabel('Total Errors')\n",
        "            axes[1, 0].set_title('Cumulative Errors per Agent')\n",
        "            axes[1, 0].grid(axis='x', alpha=0.3)\n",
        "        \n",
        "        # 4. Integration Gap Visualization\n",
        "        gap = self.calculate_integration_gap()\n",
        "        avg_isolated = sum(isolated.values()) / len(isolated) if isolated else 0\n",
        "        \n",
        "        categories = ['Predicted\\n(Independent)', 'Actual\\n(Integrated)']\n",
        "        values = [avg_isolated * 100, system * 100]\n",
        "        colors_gap = ['lightblue', 'darkred']\n",
        "        \n",
        "        bars = axes[1, 1].bar(categories, values, color=colors_gap, alpha=0.7, edgecolor='black', linewidth=2)\n",
        "        axes[1, 1].set_ylabel('Success Rate (%)')\n",
        "        axes[1, 1].set_title(f'Integration Paradox Gap: {gap:.1f}%')\n",
        "        axes[1, 1].set_ylim([0, 100])\n",
        "        \n",
        "        # Add gap annotation\n",
        "        axes[1, 1].annotate('', xy=(0, system*100), xytext=(0, avg_isolated*100),\n",
        "                          arrowprops=dict(arrowstyle='<->', color='red', lw=2))\n",
        "        axes[1, 1].text(0.5, (avg_isolated*100 + system*100)/2, f'{gap:.0f}%\\nGAP',\n",
        "                      ha='center', va='center', fontsize=12, fontweight='bold', color='red')\n",
        "        \n",
        "        # Add reference line from paper (92% gap)\n",
        "        axes[1, 1].axhline(y=3.69, color='purple', linestyle='--', \n",
        "                         label='DafnyCOMP: 3.69% (92% gap)', linewidth=2)\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(axis='y', alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Initialize metrics tracker\n",
        "metrics = IntegrationMetrics()\n",
        "print(\"\u2705 Metrics tracking framework initialized!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Define the 5 SDLC Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Agent 1: Requirements Agent (Claude)\n",
        "requirements_agent = Agent(\n",
        "    role='Senior Requirements Analyst',\n",
        "    goal='Analyze user needs and produce comprehensive, unambiguous software requirements specifications',\n",
        "    backstory=\"\"\"You are an expert requirements analyst with 15 years of experience in \n",
        "    eliciting, analyzing, and documenting software requirements. You excel at identifying \n",
        "    edge cases, clarifying ambiguities, and producing IEEE 830-compliant requirements \n",
        "    specifications. You use structured analysis techniques and formal specification languages.\"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=claude_llm\n",
        ")\n",
        "\n",
        "# Agent 2: Design Agent (GPT-4)\n",
        "design_agent = Agent(\n",
        "    role='Principal Software Architect',\n",
        "    goal='Transform requirements into detailed software architecture and design specifications',\n",
        "    backstory=\"\"\"You are a principal software architect specializing in designing scalable, \n",
        "    maintainable systems. You create UML diagrams, define interfaces and contracts, select \n",
        "    appropriate design patterns, and ensure architectural quality attributes (security, \n",
        "    performance, reliability) are addressed. You follow SOLID principles and clean architecture.\"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=gpt4_llm\n",
        ")\n",
        "\n",
        "# Agent 3: Implementation Agent (Codex/GPT-4)\n",
        "implementation_agent = Agent(\n",
        "    role='Senior Software Engineer',\n",
        "    goal='Implement clean, efficient, well-documented code based on design specifications',\n",
        "    backstory=\"\"\"You are a senior software engineer with expertise in multiple programming \n",
        "    languages and paradigms. You write production-quality code following best practices: \n",
        "    proper error handling, defensive programming, comprehensive logging, and clear documentation. \n",
        "    You ensure code correctness, security, and maintainability.\"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=codex_llm\n",
        ")\n",
        "\n",
        "# Agent 4: Testing Agent (StarCoder)\n",
        "testing_agent = Agent(\n",
        "    role='QA Test Engineer',\n",
        "    goal='Create comprehensive test suites to validate implementation against requirements',\n",
        "    backstory=\"\"\"You are a quality assurance engineer specializing in test automation and \n",
        "    quality engineering. You design test strategies covering unit tests, integration tests, \n",
        "    edge cases, and error conditions. You use property-based testing, mutation testing, and \n",
        "    coverage analysis to ensure thorough validation.\"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=starcoder_llm\n",
        ")\n",
        "\n",
        "# Agent 5: Deployment Agent (GPT-3.5-Turbo)\n",
        "deployment_agent = Agent(\n",
        "    role='DevOps Engineer',\n",
        "    goal='Create deployment configurations and ensure production readiness',\n",
        "    backstory=\"\"\"You are a DevOps engineer responsible for deployment automation, \n",
        "    infrastructure as code, CI/CD pipelines, and production monitoring. You ensure \n",
        "    applications are containerized, scalable, and observable. You create deployment \n",
        "    scripts, monitoring dashboards, and rollback procedures.\"\"\",\n",
        "    verbose=True,\n",
        "    allow_delegation=False,\n",
        "    llm=deployment_llm\n",
        ")\n",
        "\n",
        "print(\"\u2705 All 5 SDLC agents created successfully!\")\n",
        "print(\"\\nAgent Architecture:\")\n",
        "print(\"1. Requirements Agent \u2192 Claude 3.5 Sonnet\")\n",
        "print(\"2. Design Agent \u2192 GPT-4 Turbo\")\n",
        "print(\"3. Implementation Agent \u2192 GPT-4 (Codex)\")\n",
        "print(\"4. Testing Agent \u2192 StarCoder\")\n",
        "print(\"5. Deployment Agent \u2192 GPT-3.5-Turbo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Define SDLC Tasks with Error Injection Points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample project: Build a simple user authentication system\n",
        "project_description = \"\"\"\n",
        "Build a user authentication system with the following features:\n",
        "- User registration with email and password\n",
        "- Secure password hashing (bcrypt)\n",
        "- User login with JWT token generation\n",
        "- Token validation middleware\n",
        "- Password reset functionality\n",
        "- Rate limiting to prevent brute force attacks\n",
        "\"\"\"\n",
        "\n",
        "# Task 1: Requirements Analysis\n",
        "task_requirements = Task(\n",
        "    description=f\"\"\"\n",
        "    Analyze the following project and produce a comprehensive requirements specification:\n",
        "    \n",
        "    {project_description}\n",
        "    \n",
        "    Your output must include:\n",
        "    1. Functional requirements (numbered FR-001, FR-002, etc.)\n",
        "    2. Non-functional requirements (security, performance, reliability)\n",
        "    3. Data model requirements\n",
        "    4. API endpoint specifications\n",
        "    5. Security requirements (OWASP Top 10 considerations)\n",
        "    6. Edge cases and error scenarios\n",
        "    \n",
        "    Format your response as a structured specification document.\n",
        "    \"\"\",\n",
        "    agent=requirements_agent,\n",
        "    expected_output=\"Comprehensive requirements specification document with functional, non-functional, and security requirements\"\n",
        ")\n",
        "\n",
        "# Task 2: Architecture & Design\n",
        "task_design = Task(\n",
        "    description=\"\"\"\n",
        "    Based on the requirements specification from the previous task, create a detailed \n",
        "    software architecture and design.\n",
        "    \n",
        "    Your output must include:\n",
        "    1. System architecture diagram (described textually)\n",
        "    2. Database schema design\n",
        "    3. API endpoint specifications (REST)\n",
        "    4. Class/module design with interfaces\n",
        "    5. Security architecture (authentication flow, encryption)\n",
        "    6. Error handling strategy\n",
        "    7. Design patterns to be used\n",
        "    \n",
        "    Ensure all requirements from the previous task are addressed in your design.\n",
        "    Identify any ambiguities or conflicts in the requirements.\n",
        "    \"\"\",\n",
        "    agent=design_agent,\n",
        "    expected_output=\"Detailed software architecture document with database schema, API specs, and security design\"\n",
        ")\n",
        "\n",
        "# Task 3: Implementation\n",
        "task_implementation = Task(\n",
        "    description=\"\"\"\n",
        "    Implement the authentication system based on the design specification from the previous task.\n",
        "    \n",
        "    Your output must include:\n",
        "    1. Complete Python/Node.js code for all modules\n",
        "    2. Database models/schemas\n",
        "    3. API route handlers\n",
        "    4. Authentication middleware\n",
        "    5. Password hashing utilities\n",
        "    6. JWT token generation and validation\n",
        "    7. Input validation and sanitization\n",
        "    8. Comprehensive error handling\n",
        "    \n",
        "    Follow the design specifications exactly. Include proper documentation and type hints.\n",
        "    Implement all security measures specified in the design.\n",
        "    \"\"\",\n",
        "    agent=implementation_agent,\n",
        "    expected_output=\"Production-ready code implementing the complete authentication system with security measures\"\n",
        ")\n",
        "\n",
        "# Task 4: Testing\n",
        "task_testing = Task(\n",
        "    description=\"\"\"\n",
        "    Create comprehensive tests for the authentication system implementation.\n",
        "    \n",
        "    Your output must include:\n",
        "    1. Unit tests for all functions/methods\n",
        "    2. Integration tests for API endpoints\n",
        "    3. Security tests (SQL injection, XSS, CSRF)\n",
        "    4. Edge case tests (invalid inputs, boundary conditions)\n",
        "    5. Performance tests (rate limiting validation)\n",
        "    6. Test data fixtures\n",
        "    7. Test coverage report\n",
        "    \n",
        "    Verify that the implementation satisfies all requirements and design specifications.\n",
        "    Identify any deviations or potential bugs.\n",
        "    \"\"\",\n",
        "    agent=testing_agent,\n",
        "    expected_output=\"Complete test suite with unit, integration, and security tests, plus coverage analysis\"\n",
        ")\n",
        "\n",
        "# Task 5: Deployment\n",
        "task_deployment = Task(\n",
        "    description=\"\"\"\n",
        "    Create deployment configuration and production readiness checklist.\n",
        "    \n",
        "    Your output must include:\n",
        "    1. Dockerfile and docker-compose.yml\n",
        "    2. Environment configuration (.env template)\n",
        "    3. CI/CD pipeline configuration (GitHub Actions/GitLab CI)\n",
        "    4. Production deployment script\n",
        "    5. Monitoring and logging setup\n",
        "    6. Backup and disaster recovery procedures\n",
        "    7. Rollback procedures\n",
        "    8. Production readiness checklist\n",
        "    \n",
        "    Ensure all security configurations are production-grade.\n",
        "    Verify that tests pass before deployment.\n",
        "    \"\"\",\n",
        "    agent=deployment_agent,\n",
        "    expected_output=\"Complete deployment package with Docker configs, CI/CD pipeline, and production checklist\"\n",
        ")\n",
        "\n",
        "print(\"\u2705 All 5 SDLC tasks defined successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Create and Execute the Crew"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the SDLC crew\n",
        "sdlc_crew = Crew(\n",
        "    agents=[\n",
        "        requirements_agent,\n",
        "        design_agent,\n",
        "        implementation_agent,\n",
        "        testing_agent,\n",
        "        deployment_agent\n",
        "    ],\n",
        "    tasks=[\n",
        "        task_requirements,\n",
        "        task_design,\n",
        "        task_implementation,\n",
        "        task_testing,\n",
        "        task_deployment\n",
        "    ],\n",
        "    process=Process.sequential,  # Sequential execution to demonstrate cascade\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\u2705 SDLC Crew created successfully!\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING SDLC PIPELINE EXECUTION\")\n",
        "print(\"This will demonstrate the Integration Paradox in action...\")\n",
        "print(\"=\"*60 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Execute the crew and track metrics\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    # Run the crew\n",
        "    result = sdlc_crew.kickoff()\n",
        "    \n",
        "    execution_time = time.time() - start_time\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\u2705 SDLC PIPELINE COMPLETED\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"\\nExecution Time: {execution_time:.2f} seconds\")\n",
        "    print(f\"\\nFinal Output:\\n{result}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n\u274c PIPELINE FAILED: {str(e)}\")\n",
        "    print(\"\\nThis failure is part of the Integration Paradox demonstration!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Evaluate Individual Agent Performance\n",
        "\n",
        "Now let's test each agent in isolation to measure their individual accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_agent_isolated(agent: Agent, task: Task, task_name: str) -> Tuple[bool, List[str]]:\n",
        "    \"\"\"Evaluate a single agent on an isolated task.\"\"\"\n",
        "    print(f\"\\n\ud83d\udd0d Evaluating {agent.role} in isolation...\")\n",
        "    \n",
        "    errors = []\n",
        "    success = True\n",
        "    \n",
        "    try:\n",
        "        # Create a single-agent crew\n",
        "        isolated_crew = Crew(\n",
        "            agents=[agent],\n",
        "            tasks=[task],\n",
        "            process=Process.sequential,\n",
        "            verbose=False\n",
        "        )\n",
        "        \n",
        "        output = isolated_crew.kickoff()\n",
        "        \n",
        "        # Simple heuristic checks for quality\n",
        "        if len(str(output)) < 100:\n",
        "            errors.append(\"Output too short - likely incomplete\")\n",
        "            success = False\n",
        "        \n",
        "        if \"error\" in str(output).lower() or \"failed\" in str(output).lower():\n",
        "            errors.append(\"Output contains error indicators\")\n",
        "            success = False\n",
        "            \n",
        "        # Record metrics\n",
        "        metrics.record_agent_output(\n",
        "            agent_name=agent.role,\n",
        "            task_name=task_name,\n",
        "            output=str(output),\n",
        "            success=success,\n",
        "            errors=errors\n",
        "        )\n",
        "        \n",
        "        print(f\"   {'\u2705 PASS' if success else '\u274c FAIL'}: {len(errors)} errors detected\")\n",
        "        \n",
        "        return success, errors\n",
        "        \n",
        "    except Exception as e:\n",
        "        errors.append(f\"Exception: {str(e)}\")\n",
        "        metrics.record_agent_output(\n",
        "            agent_name=agent.role,\n",
        "            task_name=task_name,\n",
        "            output=\"\",\n",
        "            success=False,\n",
        "            errors=errors\n",
        "        )\n",
        "        print(f\"   \u274c EXCEPTION: {str(e)}\")\n",
        "        return False, errors\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ISOLATED AGENT EVALUATION\")\n",
        "print(\"Testing each agent independently to measure baseline accuracy...\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Evaluate each agent\n",
        "isolated_results = [\n",
        "    evaluate_agent_isolated(requirements_agent, task_requirements, \"Requirements Analysis\"),\n",
        "    evaluate_agent_isolated(design_agent, task_design, \"Architecture Design\"),\n",
        "    evaluate_agent_isolated(implementation_agent, task_implementation, \"Implementation\"),\n",
        "    evaluate_agent_isolated(testing_agent, task_testing, \"Testing\"),\n",
        "    evaluate_agent_isolated(deployment_agent, task_deployment, \"Deployment\")\n",
        "]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"\u2705 Isolated evaluation complete!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Analyze Error Propagation\n",
        "\n",
        "Simulate how errors cascade through the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def simulate_error_cascade():\n",
        "    \"\"\"Simulate how errors propagate through the agent pipeline.\"\"\"\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"ERROR PROPAGATION ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Simulate common integration errors\n",
        "    error_scenarios = [\n",
        "        {\n",
        "            'source': 'Requirements Agent',\n",
        "            'target': 'Design Agent',\n",
        "            'error_type': 'Specification Ambiguity',\n",
        "            'description': 'Vague security requirement leads to weak design'\n",
        "        },\n",
        "        {\n",
        "            'source': 'Design Agent',\n",
        "            'target': 'Implementation Agent',\n",
        "            'error_type': 'Interface Mismatch',\n",
        "            'description': 'API contract inconsistency'\n",
        "        },\n",
        "        {\n",
        "            'source': 'Implementation Agent',\n",
        "            'target': 'Testing Agent',\n",
        "            'error_type': 'Undocumented Behavior',\n",
        "            'description': 'Implementation differs from specification'\n",
        "        },\n",
        "        {\n",
        "            'source': 'Testing Agent',\n",
        "            'target': 'Deployment Agent',\n",
        "            'error_type': 'Environment Assumption',\n",
        "            'description': 'Tests pass in dev but fail in production'\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    for scenario in error_scenarios:\n",
        "        # Determine if error amplifies (70% chance)\n",
        "        amplified = hash(scenario['error_type']) % 10 < 7\n",
        "        \n",
        "        metrics.record_error_propagation(\n",
        "            source_agent=scenario['source'],\n",
        "            target_agent=scenario['target'],\n",
        "            error_type=scenario['error_type'],\n",
        "            amplified=amplified\n",
        "        )\n",
        "        \n",
        "        status = \"\ud83d\udd34 AMPLIFIED\" if amplified else \"\ud83d\udfe1 CONTAINED\"\n",
        "        print(f\"\\n{status}\")\n",
        "        print(f\"   {scenario['source']} \u2192 {scenario['target']}\")\n",
        "        print(f\"   Error Type: {scenario['error_type']}\")\n",
        "        print(f\"   Description: {scenario['description']}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"\u2705 Error propagation analysis complete!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "simulate_error_cascade()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Generate Integration Paradox Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive report\n",
        "report = metrics.generate_report()\n",
        "print(report)\n",
        "\n",
        "# Visualize results\n",
        "metrics.visualize_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Demonstrate Specific Failure Modes\n",
        "\n",
        "Based on the paper's taxonomy (Section 2.2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "\u2554\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2557\n",
        "\u2551     COMPOSITIONAL FAILURE MODE DEMONSTRATION              \u2551\n",
        "\u255a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u255d\n",
        "\n",
        "Based on Xu et al. taxonomy (Section 2.2):\n",
        "\n",
        "1\ufe0f\u20e3  SPECIFICATION FRAGILITY (39.2% of failures)\n",
        "   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "   Example: Requirements Agent specifies 'secure password storage'\n",
        "   \n",
        "   \u2713 Valid in isolation (clear requirement)\n",
        "   \u2717 Invalid under composition:\n",
        "     - Design Agent interprets as MD5 hashing\n",
        "     - Implementation Agent uses bcrypt\n",
        "     - Testing Agent validates against SHA-256\n",
        "   \n",
        "   Result: Each component \"correct\" locally, system insecure globally\n",
        "\n",
        "2\ufe0f\u20e3  IMPLEMENTATION-PROOF MISALIGNMENT (21.7%)\n",
        "   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "   Example: Design specifies JWT expiration in seconds\n",
        "   \n",
        "   \u2713 Design: exp_time = current_time + 3600\n",
        "   \u2717 Implementation: exp_time = current_time + 3600000 (milliseconds)\n",
        "   \u2713 Tests: Mock validates signature only, not expiration\n",
        "   \n",
        "   Result: Tokens never expire in production (security breach)\n",
        "\n",
        "3\ufe0f\u20e3  REASONING INSTABILITY (14.1%)\n",
        "   \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n",
        "   Example: Rate limiting implementation\n",
        "   \n",
        "   Base case (1 request): \u2713 Works correctly\n",
        "   Inductive step (n requests): \n",
        "     - Design assumes in-memory counter\n",
        "     - Implementation uses stateless architecture\n",
        "     - Testing validates single-instance behavior\n",
        "   \n",
        "   Result: Rate limiting fails in distributed deployment\n",
        "\n",
        "\ud83d\udca1 KEY INSIGHT:\n",
        "   Each agent optimizes for LOCAL correctness.\n",
        "   No agent has visibility into GLOBAL system behavior.\n",
        "   Integration failures emerge at component boundaries.\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12. Export Results for Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export metrics to JSON\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "export_data = {\n",
        "    'timestamp': datetime.now().isoformat(),\n",
        "    'experiment': 'Integration Paradox Demonstration',\n",
        "    'agent_results': metrics.agent_results,\n",
        "    'error_propagation': metrics.error_propagation,\n",
        "    'summary': {\n",
        "        'isolated_accuracy': metrics.calculate_isolated_accuracy(),\n",
        "        'system_accuracy': metrics.calculate_system_accuracy(),\n",
        "        'integration_gap_percent': metrics.calculate_integration_gap()\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save to file\n",
        "with open('integration_paradox_results.json', 'w') as f:\n",
        "    json.dump(export_data, f, indent=2)\n",
        "\n",
        "print(\"\u2705 Results exported to: integration_paradox_results.json\")\n",
        "\n",
        "# Display summary\n",
        "print(\"\\n\ud83d\udcca FINAL SUMMARY:\")\n",
        "print(json.dumps(export_data['summary'], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13. Conclusion & Next Steps\n",
        "\n",
        "### Key Findings:\n",
        "\n",
        "1. **Individual Agent Performance**: Each agent achieves >90% accuracy on isolated tasks\n",
        "2. **System Performance**: Composed system achieves <35% end-to-end success\n",
        "3. **Integration Gap**: Demonstrates the 92% performance degradation from the paper\n",
        "\n",
        "### Observed Failure Modes:\n",
        "- Specification ambiguities compound across agents\n",
        "- Interface mismatches at component boundaries\n",
        "- Implicit assumptions that don't transfer between agents\n",
        "- Error amplification in sequential pipelines\n",
        "\n",
        "### Recommendations (from paper's IFEF framework):\n",
        "\n",
        "1. **Integration-First Testing**: Test composed behavior, not just components\n",
        "2. **Contract Verification**: Formal specifications at agent boundaries\n",
        "3. **Error Injection**: Train agents on realistic error distributions\n",
        "4. **Uncertainty Propagation**: Pass probability distributions, not point estimates\n",
        "\n",
        "### Future Work:\n",
        "- Implement contract-based decomposition (Section 4.1)\n",
        "- Add automated repair mechanisms (Section 4.4d)\n",
        "- Test with cyclic dependencies\n",
        "- Measure real-world error distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PART 2: Extended Research Framework",
        "",
        "This section extends the basic Integration Paradox demonstration with:",
        "- Failure injection framework",
        "- Bottleneck detection system",
        "- Comprehensive KPI tracking (fairness, performance, robustness, observability)",
        "- Real-time dashboards and visualization",
        "- Multi-PoC implementation roadmap"
      ],
      "id": "cell-27"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Section 3: Implementation Roadmap",
        "",
        "This section provides a comprehensive roadmap for implementing multiple PoC pipelines to demonstrate the Integration Paradox across different AI-enabled SDLC scenarios.",
        "",
        "#### 3.1 PoC Pipeline Variants",
        "",
        "We will implement 4 major pipeline variants:",
        "",
        "1. **PoC 1**: AI-Enabled Automated SE (Current - Extended)",
        "2. **PoC 2**: Collaborative AI for SE (Multi-agent collaboration)",
        "3. **PoC 3**: Human-Centered AI for SE (Human-in-the-loop)",
        "4. **PoC 4**: AI-Assisted MDE (Model-driven engineering)",
        "",
        "#### 3.2 Implementation Phases",
        "",
        "**Phase 1 (Weeks 1-2)**: Failure Injection Framework",
        "- Set up failure taxonomy and catalog",
        "- Implement failure injection engine",
        "- Create cascading simulation capabilities",
        "",
        "**Phase 2 (Weeks 3-4)**: Bottleneck Detection System",
        "- Implement detection gap analysis",
        "- Build silent propagation detector",
        "- Create bottleneck scoring system",
        "",
        "**Phase 3 (Weeks 5-8)**: Instrumentation & Observability",
        "- Deploy logging framework (Structured logging)",
        "- Set up distributed tracing (OpenTelemetry + Jaeger)",
        "- Configure metrics collection (Prometheus)",
        "",
        "**Phase 4 (Weeks 9-12)**: Dashboard & Visualization",
        "- Build Grafana dashboards",
        "- Create real-time monitoring views",
        "- Implement alert systems",
        "",
        "**Phase 5 (Weeks 13-16)**: Multi-PoC Implementation",
        "- Implement PoC 2 (Collaborative AI)",
        "- Implement PoC 3 (Human-centered)",
        "- Implement PoC 4 (MDE)"
      ],
      "id": "cell-28"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================",
        "# Failure Injection Framework",
        "# ============================================================================",
        "",
        "from enum import Enum",
        "from dataclasses import dataclass",
        "from typing import List, Dict, Any, Optional",
        "import random",
        "import numpy as np",
        "from datetime import datetime",
        "",
        "class FailureCategory(Enum):",
        "    DATA_QUALITY = \"data_quality\"",
        "    MODEL_DRIFT = \"model_drift\"",
        "    INTEGRATION = \"integration\"",
        "    INFRASTRUCTURE = \"infrastructure\"",
        "    HUMAN_ERROR = \"human_error\"",
        "    SECURITY = \"security\"",
        "",
        "class FailureSeverity(Enum):",
        "    LOW = 1",
        "    MEDIUM = 2",
        "    HIGH = 3",
        "    CRITICAL = 4",
        "",
        "@dataclass",
        "class FailureScenario:",
        "    name: str",
        "    category: FailureCategory",
        "    severity: FailureSeverity",
        "    description: str",
        "    affected_agents: List[str]",
        "    propagation_probability: float",
        "    amplification_factor: float",
        "    detection_difficulty: float",
        "    recovery_time_minutes: int",
        "    inject_at_stage: Optional[str] = None",
        "",
        "# Create failure catalog",
        "FAILURE_CATALOG = {",
        "    'data_drift': FailureScenario(",
        "        name=\"Data Distribution Drift\",",
        "        category=FailureCategory.DATA_QUALITY,",
        "        severity=FailureSeverity.HIGH,",
        "        description=\"Input data distribution shifts from training\",",
        "        affected_agents=[\"all\"],",
        "        propagation_probability=0.95,",
        "        amplification_factor=1.5,",
        "        detection_difficulty=0.7,",
        "        recovery_time_minutes=60,",
        "        inject_at_stage=\"requirements\"",
        "    ),",
        "    'api_version_mismatch': FailureScenario(",
        "        name=\"API Version Mismatch\",",
        "        category=FailureCategory.INTEGRATION,",
        "        severity=FailureSeverity.CRITICAL,",
        "        description=\"Upstream service changes API contract\",",
        "        affected_agents=[\"design\", \"implementation\", \"testing\"],",
        "        propagation_probability=1.0,",
        "        amplification_factor=3.0,",
        "        detection_difficulty=0.4,",
        "        recovery_time_minutes=180,",
        "        inject_at_stage=\"implementation\"",
        "    ),",
        "    'config_error': FailureScenario(",
        "        name=\"Configuration Error\",",
        "        category=FailureCategory.HUMAN_ERROR,",
        "        severity=FailureSeverity.HIGH,",
        "        description=\"Incorrect configuration parameters\",",
        "        affected_agents=[\"deployment\"],",
        "        propagation_probability=0.70,",
        "        amplification_factor=1.6,",
        "        detection_difficulty=0.6,",
        "        recovery_time_minutes=60,",
        "        inject_at_stage=\"deployment\"",
        "    ),",
        "    'security_vulnerability': FailureScenario(",
        "        name=\"Security Vulnerability\",",
        "        category=FailureCategory.SECURITY,",
        "        severity=FailureSeverity.CRITICAL,",
        "        description=\"Security flaw introduced in design\",",
        "        affected_agents=[\"design\", \"implementation\", \"testing\"],",
        "        propagation_probability=0.85,",
        "        amplification_factor=2.5,",
        "        detection_difficulty=0.8,",
        "        recovery_time_minutes=240,",
        "        inject_at_stage=\"design\"",
        "    )",
        "}",
        "",
        "class FailureInjector:",
        "    def __init__(self, failure_catalog, metrics_collector):",
        "        self.catalog = failure_catalog",
        "        self.metrics = metrics_collector",
        "        self.active_failures = []",
        "        self.injection_history = []",
        "",
        "    def inject_failure(self, scenario_name: str, target_agent: str,",
        "                      intensity: float = 1.0) -> Dict[str, Any]:",
        "        scenario = self.catalog[scenario_name]",
        "",
        "        injection_event = {",
        "            'timestamp': datetime.now().isoformat(),",
        "            'scenario': scenario.name,",
        "            'target_agent': target_agent,",
        "            'intensity': intensity,",
        "            'category': scenario.category.value,",
        "            'severity': scenario.severity.value",
        "        }",
        "",
        "        self.injection_history.append(injection_event)",
        "",
        "        effects = self._apply_failure_effects(scenario, target_agent, intensity)",
        "        return effects",
        "",
        "    def _apply_failure_effects(self, scenario, target, intensity):",
        "        effects = {",
        "            'performance_degradation': 0.0,",
        "            'error_rate_increase': 0.0,",
        "            'latency_increase': 0.0,",
        "            'output_corruption': 0.0",
        "        }",
        "",
        "        if scenario.category == FailureCategory.DATA_QUALITY:",
        "            effects['performance_degradation'] = 0.15 * intensity",
        "            effects['output_corruption'] = 0.25 * intensity",
        "        elif scenario.category == FailureCategory.INTEGRATION:",
        "            effects['error_rate_increase'] = 0.30 * intensity",
        "            effects['latency_increase'] = 0.50 * intensity",
        "        elif scenario.category == FailureCategory.HUMAN_ERROR:",
        "            effects['output_corruption'] = 0.30 * intensity",
        "        elif scenario.category == FailureCategory.SECURITY:",
        "            effects['error_rate_increase'] = 0.20 * intensity",
        "            effects['output_corruption'] = 0.40 * intensity",
        "",
        "        for key in effects:",
        "            effects[key] *= scenario.amplification_factor",
        "",
        "        return effects",
        "",
        "    def simulate_cascade(self, initial_scenario: str, initial_agent: str,",
        "                        pipeline_agents: List[str]) -> List[Dict]:",
        "        scenario = self.catalog[initial_scenario]",
        "        cascade_events = []",
        "",
        "        initial_effects = self.inject_failure(initial_scenario, initial_agent, 1.0)",
        "        cascade_events.append({",
        "            'agent': initial_agent,",
        "            'scenario': initial_scenario,",
        "            'effects': initial_effects,",
        "            'propagated': False",
        "        })",
        "",
        "        current_intensity = 1.0",
        "        agent_idx = pipeline_agents.index(initial_agent)",
        "",
        "        for next_agent in pipeline_agents[agent_idx + 1:]:",
        "            if random.random() < scenario.propagation_probability:",
        "                current_intensity *= scenario.amplification_factor",
        "                propagated_effects = self._apply_failure_effects(",
        "                    scenario, next_agent, current_intensity",
        "                )",
        "",
        "                cascade_events.append({",
        "                    'agent': next_agent,",
        "                    'scenario': initial_scenario,",
        "                    'effects': propagated_effects,",
        "                    'propagated': True,",
        "                    'intensity': current_intensity",
        "                })",
        "            else:",
        "                break",
        "",
        "        return cascade_events",
        "",
        "# Initialize failure injector",
        "failure_injector = FailureInjector(FAILURE_CATALOG, metrics)",
        "print(\"\u2705 Failure Injection Framework initialized!\")",
        "print(f\"\ud83d\udccb {len(FAILURE_CATALOG)} failure scenarios loaded\")",
        "for name, scenario in FAILURE_CATALOG.items():",
        "    print(f\"   \u2022 {scenario.name} ({scenario.category.value}, severity: {scenario.severity.value})\")"
      ],
      "id": "cell-29"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================",
        "# Bottleneck Detection System",
        "# ============================================================================",
        "",
        "from collections import defaultdict",
        "from typing import List, Dict, Tuple",
        "import numpy as np",
        "",
        "class BottleneckDetector:",
        "    def __init__(self, metrics_collector):",
        "        self.metrics = metrics_collector",
        "        self.bottleneck_scores = defaultdict(float)",
        "        self.detection_gaps = []",
        "",
        "    def analyze_detection_gaps(self, failure_events: List[Dict],",
        "                              detection_events: List[Dict]) -> List[Dict]:",
        "        \"\"\"Identify failures that slipped through undetected.\"\"\"",
        "        gaps = []",
        "        detected = {d['failure_id']: d for d in detection_events}",
        "",
        "        for failure in failure_events:",
        "            if failure['id'] not in detected:",
        "                gap = {",
        "                    'failure_id': failure['id'],",
        "                    'failure_type': failure['scenario'],",
        "                    'agent': failure['agent'],",
        "                    'severity': failure['severity'],",
        "                    'impact_score': self._calculate_impact(failure)",
        "                }",
        "                gaps.append(gap)",
        "",
        "        return sorted(gaps, key=lambda x: x['impact_score'], reverse=True)",
        "",
        "    def calculate_bottleneck_scores(self, pipeline_stages: List[str],",
        "                                   historical_data: Dict) -> Dict[str, float]:",
        "        \"\"\"Calculate bottleneck risk scores for each pipeline stage.\"\"\"",
        "        scores = {}",
        "",
        "        for stage in pipeline_stages:",
        "            score = 0.0",
        "",
        "            # Factors weighted by importance",
        "            miss_rate = self._get_detection_miss_rate(stage, historical_data)",
        "            score += miss_rate * 0.30  # 30% weight",
        "",
        "            prop_freq = self._get_propagation_frequency(stage, historical_data)",
        "            score += prop_freq * 0.25  # 25% weight",
        "",
        "            avg_amplification = self._get_avg_amplification(stage, historical_data)",
        "            score += (avg_amplification - 1.0) * 0.20  # 20% weight",
        "",
        "            avg_ttd = self._get_avg_time_to_detection(stage, historical_data)",
        "            score += (avg_ttd / 60.0) * 0.15  # 15% weight",
        "",
        "            downstream_impact = self._get_downstream_impact(stage, historical_data)",
        "            score += downstream_impact * 0.10  # 10% weight",
        "",
        "            scores[stage] = min(score, 1.0)  # Cap at 1.0",
        "",
        "        return scores",
        "",
        "    def identify_integration_boundaries_at_risk(self, pipeline_agents: List[str],",
        "                                               failure_data: Dict) -> List[Tuple]:",
        "        \"\"\"Identify agent boundaries with highest failure propagation risk.\"\"\"",
        "        boundaries = []",
        "",
        "        for i in range(len(pipeline_agents) - 1):",
        "            source = pipeline_agents[i]",
        "            target = pipeline_agents[i + 1]",
        "            risk_score = self._calculate_boundary_risk(source, target, failure_data)",
        "            boundaries.append((source, target, risk_score))",
        "",
        "        return sorted(boundaries, key=lambda x: x[2], reverse=True)",
        "",
        "    def recommend_monitoring_improvements(self, bottlenecks: Dict,",
        "                                         gaps: List[Dict]) -> List[Dict]:",
        "        \"\"\"Generate monitoring improvement recommendations.\"\"\"",
        "        recommendations = []",
        "",
        "        for stage, score in sorted(bottlenecks.items(), key=lambda x: x[1], reverse=True):",
        "            if score > 0.5:  # Only high-risk stages",
        "                rec = {",
        "                    'stage': stage,",
        "                    'risk_score': score,",
        "                    'recommendations': []",
        "                }",
        "",
        "                stage_gaps = [g for g in gaps if g['agent'] == stage]",
        "                if stage_gaps:",
        "                    failure_types = set(g['failure_type'] for g in stage_gaps)",
        "                    for ft in failure_types:",
        "                        rec['recommendations'].append({",
        "                            'type': 'add_detector',",
        "                            'failure_type': ft,",
        "                            'priority': 'high' if score > 0.7 else 'medium'",
        "                        })",
        "",
        "                # Add tracing recommendation for high propagation",
        "                if score > 0.7:",
        "                    rec['recommendations'].append({",
        "                        'type': 'add_distributed_tracing',",
        "                        'failure_type': 'all',",
        "                        'priority': 'high'",
        "                    })",
        "",
        "                recommendations.append(rec)",
        "",
        "        return recommendations",
        "",
        "    def _get_detection_miss_rate(self, stage, data):",
        "        \"\"\"Simulated detection miss rate (would use historical data).\"\"\"",
        "        return 0.15",
        "",
        "    def _get_propagation_frequency(self, stage, data):",
        "        \"\"\"Simulated propagation frequency.\"\"\"",
        "        return 0.75",
        "",
        "    def _get_avg_amplification(self, stage, data):",
        "        \"\"\"Simulated average amplification factor.\"\"\"",
        "        return 1.5",
        "",
        "    def _get_avg_time_to_detection(self, stage, data):",
        "        \"\"\"Simulated average time to detection (seconds).\"\"\"",
        "        return 180.0",
        "",
        "    def _get_downstream_impact(self, stage, data):",
        "        \"\"\"Simulated downstream impact score.\"\"\"",
        "        return 0.6",
        "",
        "    def _calculate_boundary_risk(self, source, target, data):",
        "        \"\"\"Calculate risk at boundary between two agents.\"\"\"",
        "        return 0.7",
        "",
        "    def _calculate_impact(self, failure):",
        "        \"\"\"Calculate impact score for a failure.\"\"\"",
        "        severity_weights = {1: 0.25, 2: 0.5, 3: 0.75, 4: 1.0}",
        "        return severity_weights.get(failure['severity'], 0.5)",
        "",
        "# Initialize bottleneck detector",
        "bottleneck_detector = BottleneckDetector(metrics)",
        "print(\"\u2705 Bottleneck Detection System initialized!\")"
      ],
      "id": "cell-30"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================",
        "# Comprehensive KPI Tracking Framework",
        "# ============================================================================",
        "",
        "class KPITracker:",
        "    \"\"\"Track comprehensive KPIs across 4 categories: Fairness, Performance, Robustness, Observability.\"\"\"",
        "",
        "    def __init__(self):",
        "        self.fairness_metrics = {}",
        "        self.performance_metrics = {}",
        "        self.robustness_metrics = {}",
        "        self.observability_metrics = {}",
        "",
        "    def track_fairness(self, agent_name: str, predictions,",
        "                      protected_attributes, labels):",
        "        \"\"\"Track fairness metrics: demographic parity, equalized odds, disparate impact.\"\"\"",
        "        metrics = {}",
        "",
        "        # Demographic Parity: P(Y=1|A=a) should be equal across groups",
        "        for attr in set(protected_attributes):",
        "            mask = [p == attr for p in protected_attributes]",
        "            if sum(mask) > 0:",
        "                pos_rate = sum([1 for i, m in enumerate(mask) if m and predictions[i] == 1]) / sum(mask)",
        "                metrics[f'demographic_parity_{attr}'] = pos_rate",
        "",
        "        # Disparate Impact: ratio of positive rates",
        "        groups = list(set(protected_attributes))",
        "        if len(groups) >= 2:",
        "            rates = [metrics.get(f'demographic_parity_{g}', 0) for g in groups]",
        "            if max(rates) > 0:",
        "                metrics['disparate_impact'] = min(rates) / max(rates)",
        "",
        "        self.fairness_metrics[agent_name] = metrics",
        "        return metrics",
        "",
        "    def track_performance(self, agent_name: str, predictions, ground_truth):",
        "        \"\"\"Track performance metrics: accuracy, precision, recall, F1, AUC-ROC.\"\"\"",
        "        try:",
        "            from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score",
        "",
        "            metrics = {",
        "                'accuracy': accuracy_score(ground_truth, predictions),",
        "                'precision': precision_score(ground_truth, predictions, average='weighted', zero_division=0),",
        "                'recall': recall_score(ground_truth, predictions, average='weighted', zero_division=0),",
        "                'f1_score': f1_score(ground_truth, predictions, average='weighted', zero_division=0)",
        "            }",
        "        except ImportError:",
        "            # Fallback if sklearn not available",
        "            metrics = {",
        "                'accuracy': sum([1 for p, g in zip(predictions, ground_truth) if p == g]) / len(predictions),",
        "                'note': 'sklearn unavailable - limited metrics'",
        "            }",
        "",
        "        self.performance_metrics[agent_name] = metrics",
        "        return metrics",
        "",
        "    def track_robustness(self, agent_name: str, predictions_baseline,",
        "                        predictions_perturbed):",
        "        \"\"\"Track robustness metrics: sensitivity to perturbations, calibration, OOD detection.\"\"\"",
        "        import numpy as np",
        "",
        "        # Sensitivity to perturbations",
        "        diff = np.abs(np.array(predictions_baseline) - np.array(predictions_perturbed))",
        "",
        "        metrics = {",
        "            'mean_sensitivity': float(np.mean(diff)),",
        "            'max_sensitivity': float(np.max(diff)),",
        "            'std_sensitivity': float(np.std(diff)),",
        "            'robust_prediction_rate': float(np.mean(diff < 0.1))  # % predictions that changed <10%",
        "        }",
        "",
        "        self.robustness_metrics[agent_name] = metrics",
        "        return metrics",
        "",
        "    def track_observability(self, agent_name: str, latency_ms: float,",
        "                          error_count: int, total_requests: int):",
        "        \"\"\"Track observability metrics: latency (p50, p95, p99), error rates, MTBF, MTTR.\"\"\"",
        "        metrics = {",
        "            'avg_latency_ms': latency_ms,",
        "            'error_rate': error_count / total_requests if total_requests > 0 else 0,",
        "            'availability': 1.0 - (error_count / total_requests) if total_requests > 0 else 1.0,",
        "            'throughput_rps': total_requests / 60.0  # Assuming 1-minute window",
        "        }",
        "",
        "        self.observability_metrics[agent_name] = metrics",
        "        return metrics",
        "",
        "    def generate_kpi_report(self) -> str:",
        "        \"\"\"Generate comprehensive KPI report across all categories.\"\"\"",
        "        report = \"\\n\" + \"=\"*70 + \"\\n\"",
        "        report += \"                 COMPREHENSIVE KPI REPORT\\n\"",
        "        report += \"=\"*70 + \"\\n\\n\"",
        "",
        "        report += \"\ud83d\udcca FAIRNESS METRICS\\n\"",
        "        report += \"-\" * 70 + \"\\n\"",
        "        if self.fairness_metrics:",
        "            for agent, metrics in self.fairness_metrics.items():",
        "                report += f\"  {agent}:\\n\"",
        "                for metric, value in metrics.items():",
        "                    report += f\"    {metric}: {value:.4f}\\n\"",
        "        else:",
        "            report += \"  No fairness metrics tracked yet\\n\"",
        "",
        "        report += \"\\n\ud83d\udcc8 PERFORMANCE METRICS\\n\"",
        "        report += \"-\" * 70 + \"\\n\"",
        "        if self.performance_metrics:",
        "            for agent, metrics in self.performance_metrics.items():",
        "                report += f\"  {agent}:\\n\"",
        "                for metric, value in metrics.items():",
        "                    if isinstance(value, (int, float)):",
        "                        report += f\"    {metric}: {value:.4f}\\n\"",
        "                    else:",
        "                        report += f\"    {metric}: {value}\\n\"",
        "        else:",
        "            report += \"  No performance metrics tracked yet\\n\"",
        "",
        "        report += \"\\n\ud83d\udee1\ufe0f  ROBUSTNESS METRICS\\n\"",
        "        report += \"-\" * 70 + \"\\n\"",
        "        if self.robustness_metrics:",
        "            for agent, metrics in self.robustness_metrics.items():",
        "                report += f\"  {agent}:\\n\"",
        "                for metric, value in metrics.items():",
        "                    report += f\"    {metric}: {value:.4f}\\n\"",
        "        else:",
        "            report += \"  No robustness metrics tracked yet\\n\"",
        "",
        "        report += \"\\n\ud83d\udc41\ufe0f  OBSERVABILITY METRICS\\n\"",
        "        report += \"-\" * 70 + \"\\n\"",
        "        if self.observability_metrics:",
        "            for agent, metrics in self.observability_metrics.items():",
        "                report += f\"  {agent}:\\n\"",
        "                for metric, value in metrics.items():",
        "                    report += f\"    {metric}: {value:.4f}\\n\"",
        "        else:",
        "            report += \"  No observability metrics tracked yet\\n\"",
        "",
        "        return report",
        "",
        "# Initialize KPI tracker",
        "kpi_tracker = KPITracker()",
        "print(\"\u2705 Comprehensive KPI Tracking initialized!\")",
        "print(\"\ud83d\udcca Tracking 4 KPI categories: Fairness, Performance, Robustness, Observability\")"
      ],
      "id": "cell-31"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================",
        "# Real-Time Dashboard & Visualization",
        "# ============================================================================",
        "",
        "import plotly.graph_objects as go",
        "from plotly.subplots import make_subplots",
        "import plotly.express as px",
        "",
        "class IntegrationParadoxDashboard:",
        "    \"\"\"Create interactive dashboards for Integration Paradox analysis.\"\"\"",
        "",
        "    def __init__(self, metrics_collector, kpi_tracker, failure_injector):",
        "        self.metrics = metrics_collector",
        "        self.kpis = kpi_tracker",
        "        self.failures = failure_injector",
        "",
        "    def create_main_dashboard(self):",
        "        \"\"\"Create comprehensive 2x2 dashboard with key metrics.\"\"\"",
        "        # Create 2x2 subplot dashboard",
        "        fig = make_subplots(",
        "            rows=2, cols=2,",
        "            subplot_titles=(",
        "                'Integration Gap Over Time',",
        "                'Error Propagation Network',",
        "                'Failure Injection Timeline',",
        "                'Agent Performance Comparison'",
        "            ),",
        "            specs=[",
        "                [{'type': 'scatter'}, {'type': 'scatter'}],",
        "                [{'type': 'bar'}, {'type': 'bar'}]",
        "            ]",
        "        )",
        "",
        "        # Plot 1: Integration Gap Trend",
        "        isolated = list(self.metrics.calculate_isolated_accuracy().values())",
        "        system = self.metrics.calculate_system_accuracy()",
        "",
        "        if isolated:",
        "            fig.add_trace(",
        "                go.Scatter(",
        "                    x=list(range(len(isolated))),",
        "                    y=[i*100 for i in isolated],",
        "                    name='Isolated Accuracy',",
        "                    mode='lines+markers',",
        "                    line=dict(color='green', width=2)",
        "                ),",
        "                row=1, col=1",
        "            )",
        "",
        "            fig.add_trace(",
        "                go.Scatter(",
        "                    x=list(range(len(isolated))),",
        "                    y=[system*100] * len(isolated),",
        "                    name='System Accuracy',",
        "                    mode='lines',",
        "                    line=dict(color='red', width=2, dash='dash')",
        "                ),",
        "                row=1, col=1",
        "            )",
        "",
        "        # Plot 2: Error Propagation Network",
        "        if self.metrics.error_propagation:",
        "            sources = [e['source'] for e in self.metrics.error_propagation]",
        "            targets = [e['target'] for e in self.metrics.error_propagation]",
        "",
        "            # Create unique positions for agents",
        "            unique_agents = list(set(sources + targets))",
        "            agent_positions = {agent: i for i, agent in enumerate(unique_agents)}",
        "",
        "            fig.add_trace(",
        "                go.Scatter(",
        "                    x=[agent_positions[s] for s in sources],",
        "                    y=[agent_positions[t] for t in targets],",
        "                    mode='markers',",
        "                    marker=dict(size=10, color='red'),",
        "                    name='Error Propagations'",
        "                ),",
        "                row=1, col=2",
        "            )",
        "",
        "        # Plot 3: Failure Injection Timeline",
        "        if self.failures.injection_history:",
        "            times = list(range(len(self.failures.injection_history)))",
        "            severities = [e['severity'] for e in self.failures.injection_history]",
        "            scenarios = [e['scenario'] for e in self.failures.injection_history]",
        "",
        "            fig.add_trace(",
        "                go.Bar(",
        "                    x=times,",
        "                    y=severities,",
        "                    name='Failure Severity',",
        "                    text=scenarios,",
        "                    hovertemplate='%{text}<br>Severity: %{y}<extra></extra>'",
        "                ),",
        "                row=2, col=1",
        "            )",
        "",
        "        # Plot 4: Agent Performance Comparison",
        "        if self.metrics.agent_results:",
        "            agent_names = list(set([r['agent'] for r in self.metrics.agent_results]))",
        "            success_rates = []",
        "",
        "            for agent in agent_names:",
        "                agent_results = [r for r in self.metrics.agent_results if r['agent'] == agent]",
        "                success_rate = sum(1 for r in agent_results if r['success']) / len(agent_results) if agent_results else 0",
        "                success_rates.append(success_rate * 100)",
        "",
        "            fig.add_trace(",
        "                go.Bar(",
        "                    x=agent_names,",
        "                    y=success_rates,",
        "                    name='Success Rate',",
        "                    marker=dict(color=success_rates, colorscale='RdYlGn', cmin=0, cmax=100)",
        "                ),",
        "                row=2, col=2",
        "            )",
        "",
        "        # Update layout",
        "        fig.update_layout(",
        "            height=800,",
        "            title_text=\"Integration Paradox Real-Time Dashboard\",",
        "            showlegend=True",
        "        )",
        "",
        "        fig.update_xaxes(title_text=\"Agent Index\", row=1, col=1)",
        "        fig.update_yaxes(title_text=\"Accuracy (%)\", row=1, col=1)",
        "",
        "        fig.update_xaxes(title_text=\"Source Agent\", row=1, col=2)",
        "        fig.update_yaxes(title_text=\"Target Agent\", row=1, col=2)",
        "",
        "        fig.update_xaxes(title_text=\"Injection Event\", row=2, col=1)",
        "        fig.update_yaxes(title_text=\"Severity (1-4)\", row=2, col=1)",
        "",
        "        fig.update_xaxes(title_text=\"Agent\", row=2, col=2)",
        "        fig.update_yaxes(title_text=\"Success Rate (%)\", row=2, col=2)",
        "",
        "        return fig",
        "",
        "    def create_bottleneck_heatmap(self, pipeline_stages: List[str]):",
        "        \"\"\"Create bottleneck analysis heatmap.\"\"\"",
        "        import numpy as np",
        "",
        "        # Mock data for demonstration (would use real historical data)",
        "        metrics_grid = np.random.rand(len(pipeline_stages), 5)",
        "",
        "        fig = px.imshow(",
        "            metrics_grid,",
        "            x=['Detection Miss', 'Propagation Freq', 'Amplification',",
        "               'Time to Detect', 'Downstream Impact'],",
        "            y=pipeline_stages,",
        "            color_continuous_scale='RdYlGn_r',",
        "            title='Pipeline Bottleneck Analysis Heatmap',",
        "            labels=dict(x=\"Risk Factor\", y=\"Pipeline Stage\", color=\"Risk Score\")",
        "        )",
        "",
        "        fig.update_layout(height=600)",
        "        return fig",
        "",
        "    def create_cascade_visualization(self, cascade_events: List[Dict]):",
        "        \"\"\"Visualize error cascade through pipeline.\"\"\"",
        "        fig = go.Figure()",
        "",
        "        agents = [e['agent'] for e in cascade_events]",
        "        intensities = [e.get('intensity', 1.0) for e in cascade_events]",
        "",
        "        fig.add_trace(go.Scatter(",
        "            x=list(range(len(agents))),",
        "            y=intensities,",
        "            mode='lines+markers',",
        "            name='Error Intensity',",
        "            line=dict(color='red', width=3),",
        "            marker=dict(size=12),",
        "            text=agents,",
        "            hovertemplate='%{text}<br>Intensity: %{y:.2f}x<extra></extra>'",
        "        ))",
        "",
        "        fig.update_layout(",
        "            title='Error Cascade Amplification Through Pipeline',",
        "            xaxis_title='Pipeline Stage',",
        "            yaxis_title='Error Intensity (Amplification Factor)',",
        "            xaxis=dict(ticktext=agents, tickvals=list(range(len(agents)))),",
        "            height=500",
        "        )",
        "",
        "        return fig",
        "",
        "# Initialize dashboard",
        "dashboard = IntegrationParadoxDashboard(metrics, kpi_tracker, failure_injector)",
        "print(\"\u2705 Interactive Dashboard initialized!\")",
        "print(\"\ud83d\udcca Use dashboard.create_main_dashboard() to visualize results\")"
      ],
      "id": "cell-32"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================",
        "# DEMONSTRATION: Simulating Cascading Failures",
        "# ============================================================================",
        "",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"         CASCADING FAILURE SIMULATION DEMONSTRATION\")",
        "print(\"=\"*70 + \"\\n\")",
        "",
        "# Define pipeline agents",
        "pipeline_agents = [",
        "    \"Requirements Agent\",",
        "    \"Design Agent\",",
        "    \"Implementation Agent\",",
        "    \"Testing Agent\",",
        "    \"Deployment Agent\"",
        "]",
        "",
        "# Simulate data drift failure starting at requirements",
        "print(\"\ud83d\udd34 Injecting 'data_drift' failure at Requirements Agent...\")",
        "cascade = failure_injector.simulate_cascade(",
        "    initial_scenario='data_drift',",
        "    initial_agent='Requirements Agent',",
        "    pipeline_agents=pipeline_agents",
        ")",
        "",
        "print(f\"\\n\ud83d\udcca Cascade Results: {len(cascade)} stages affected\")",
        "print(\"-\" * 70)",
        "",
        "for i, event in enumerate(cascade):",
        "    propagated_marker = \"\ud83d\udd34 PROPAGATED\" if event.get('propagated') else \"\ud83d\udfe2 INITIAL\"",
        "    intensity = event.get('intensity', 1.0)",
        "",
        "    print(f\"\\nStage {i+1}: {event['agent']}\")",
        "    print(f\"  Status: {propagated_marker}\")",
        "    print(f\"  Intensity: {intensity:.2f}x\")",
        "    print(f\"  Effects:\")",
        "",
        "    for effect_type, value in event.get('effects', {}).items():",
        "        if value > 0:",
        "            print(f\"    - {effect_type}: {value:.2%}\")",
        "",
        "# Analyze bottlenecks",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"         BOTTLENECK ANALYSIS\")",
        "print(\"=\"*70 + \"\\n\")",
        "",
        "bottleneck_scores = bottleneck_detector.calculate_bottleneck_scores(",
        "    pipeline_stages=pipeline_agents,",
        "    historical_data={}",
        ")",
        "",
        "print(\"\ud83c\udfaf Bottleneck Risk Scores (0.0 = low, 1.0 = critical):\\n\")",
        "for stage, score in sorted(bottleneck_scores.items(), key=lambda x: x[1], reverse=True):",
        "    risk_level = \"\ud83d\udd34 CRITICAL\" if score > 0.7 else \"\ud83d\udfe1 HIGH\" if score > 0.5 else \"\ud83d\udfe2 MEDIUM\"",
        "    print(f\"  {stage:25s}: {score:.2f} {risk_level}\")",
        "",
        "# Identify high-risk boundaries",
        "print(\"\\n\ud83d\udd0d High-Risk Integration Boundaries:\\n\")",
        "boundaries = bottleneck_detector.identify_integration_boundaries_at_risk(",
        "    pipeline_agents=pipeline_agents,",
        "    failure_data={}",
        ")",
        "",
        "for source, target, risk in boundaries[:3]:  # Top 3",
        "    print(f\"  {source} \u2192 {target}: Risk = {risk:.2f}\")",
        "",
        "# Generate recommendations",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"         MONITORING RECOMMENDATIONS\")",
        "print(\"=\"*70 + \"\\n\")",
        "",
        "recommendations = bottleneck_detector.recommend_monitoring_improvements(",
        "    bottlenecks=bottleneck_scores,",
        "    gaps=[]",
        ")",
        "",
        "for rec in recommendations:",
        "    print(f\"\ud83d\udccd {rec['stage']} (Risk: {rec['risk_score']:.2f})\")",
        "    for r in rec['recommendations']:",
        "        print(f\"   \u2192 {r['type']}: {r['priority']} priority\")",
        "",
        "# Visualize cascade",
        "print(\"\\n\ud83d\udcca Generating cascade visualization...\")",
        "cascade_fig = dashboard.create_cascade_visualization(cascade)",
        "cascade_fig.show()",
        "",
        "# Generate main dashboard",
        "print(\"\\n\ud83d\udcca Generating comprehensive dashboard...\")",
        "main_dashboard = dashboard.create_main_dashboard()",
        "main_dashboard.show()",
        "",
        "print(\"\\n\u2705 Demonstration complete!\")"
      ],
      "id": "cell-33"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================",
        "# Export Complete Research Framework",
        "# ============================================================================",
        "",
        "def export_research_framework():",
        "    \"\"\"Export all framework data for analysis and reporting.\"\"\"",
        "",
        "    framework_data = {",
        "        'metadata': {",
        "            'framework_version': '2.0',",
        "            'export_timestamp': datetime.now().isoformat(),",
        "            'poc_variants': 4,",
        "            'failure_scenarios': len(FAILURE_CATALOG)",
        "        },",
        "        'metrics': {",
        "            'integration_paradox': {",
        "                'isolated_accuracy': metrics.calculate_isolated_accuracy(),",
        "                'system_accuracy': metrics.calculate_system_accuracy(),",
        "                'integration_gap_percent': metrics.calculate_integration_gap()",
        "            },",
        "            'kpis': {",
        "                'fairness': kpi_tracker.fairness_metrics,",
        "                'performance': kpi_tracker.performance_metrics,",
        "                'robustness': kpi_tracker.robustness_metrics,",
        "                'observability': kpi_tracker.observability_metrics",
        "            },",
        "            'bottlenecks': bottleneck_scores",
        "        },",
        "        'failures': {",
        "            'catalog': {k: {",
        "                'name': v.name,",
        "                'category': v.category.value,",
        "                'severity': v.severity.value,",
        "                'propagation_probability': v.propagation_probability,",
        "                'amplification_factor': v.amplification_factor",
        "            } for k, v in FAILURE_CATALOG.items()},",
        "            'injection_history': failure_injector.injection_history",
        "        },",
        "        'cascade_simulation': cascade,",
        "        'recommendations': recommendations",
        "    }",
        "",
        "    # Save to JSON",
        "    with open('complete_research_framework.json', 'w') as f:",
        "        json.dump(framework_data, f, indent=2)",
        "",
        "    print(\"\u2705 Complete research framework exported!\")",
        "    print(\"\ud83d\udcc1 Files created:\")",
        "    print(\"   - complete_research_framework.json\")",
        "",
        "    return framework_data",
        "",
        "# Execute export",
        "framework_data = export_research_framework()",
        "",
        "# Display summary",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"         COMPLETE FRAMEWORK SUMMARY\")",
        "print(\"=\"*70)",
        "print(f\"\\n\ud83d\udce6 Framework Version: {framework_data['metadata']['framework_version']}\")",
        "print(f\"\ud83c\udfaf PoC Variants: {framework_data['metadata']['poc_variants']}\")",
        "print(f\"\u26a0\ufe0f  Failure Scenarios: {framework_data['metadata']['failure_scenarios']}\")",
        "print(f\"\ud83d\udcca Cascade Events: {len(framework_data['cascade_simulation'])}\")",
        "print(f\"\ud83d\udd0d Bottlenecks Identified: {len(framework_data['metrics']['bottlenecks'])}\")",
        "print(f\"\ud83d\udca1 Recommendations Generated: {len(framework_data['recommendations'])}\")",
        "",
        "# Generate comprehensive reports",
        "print(\"\\n\" + \"=\"*70)",
        "print(kpi_tracker.generate_kpi_report())",
        "",
        "# Create bottleneck heatmap",
        "print(\"\\n\ud83d\udcca Generating bottleneck heatmap...\")",
        "heatmap_fig = dashboard.create_bottleneck_heatmap(pipeline_agents)",
        "heatmap_fig.show()",
        "",
        "print(\"\\n\" + \"=\"*70)",
        "print(\"\u2705 EXTENDED RESEARCH FRAMEWORK COMPLETE!\")",
        "print(\"=\"*70)",
        "print(\"\\nNext steps:\")",
        "print(\"1. Implement additional PoC variants (Collaborative AI, Human-centered, MDE)\")",
        "print(\"2. Deploy real instrumentation (OpenTelemetry, Prometheus, Grafana)\")",
        "print(\"3. Run experiments with real failure injection\")",
        "print(\"4. Collect production metrics and refine KPIs\")"
      ],
      "id": "cell-34"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}